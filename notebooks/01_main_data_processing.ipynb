{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0349a45-46f7-49ca-bf6f-e8be5113834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Matheus Gomes Correia\n",
    "# License: CC-BY-NC-SA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145ef4f-f808-496e-9973-61cb5d84e3ac",
   "metadata": {},
   "source": [
    "# Section 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0d869e-3501-4e85-972c-d5aaa0c7b715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/mapillary/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import mapillary.interface as mly\n",
    "import requests\n",
    "import json\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "import fiona\n",
    "from packaging import version\n",
    "import warnings\n",
    "import os\n",
    "from contextlib import redirect_stderr\n",
    "import logging\n",
    "from fiona.errors import DriverError\n",
    "import re\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import unicodedata\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import urllib.parse\n",
    "import ast \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0818f8c-289e-4907-920a-8f6f5e7bee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Filter out DeprecationWarnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Configure logging\n",
    "#logging.basicConfig(level=logging.WARNING)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668e00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal filter\n",
    "existed_at = '2018-01-01 00:00:00'\n",
    "existed_before = '2090-12-31 23:59:59'\n",
    "\n",
    "way_tags = {\n",
    "        \"crossing\": [\"uncontrolled\", \"traffic_signals\", \"unmarked\"],\n",
    "        \"highway\": [\"traffic_signals\", \"stop\", \"give_way\"]\n",
    "    }\n",
    "traffic_sign_tags = {\"traffic_sign\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0056d399-1bcc-4160-9c8c-4dfd13cdbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Nominatim geocoder\n",
    "geolocator = Nominatim(user_agent=\"cav-assessment-matheus-correia\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1) #rate limit\n",
    "\n",
    "# --- Fallback Dictionary (Option 1) ---\n",
    "place_to_country = {\n",
    "    \"nuku'alofa\": \"TO\",\n",
    "    \"apia\": \"WS\",\n",
    "    \"honiara\": \"SB\",\n",
    "    \"reykjavik\": \"IS\",\n",
    "    \"suva\": \"FJ\",\n",
    "    \"national capital district\": \"PG\",\n",
    "    \"wellington city\": \"NZ\",\n",
    "    \"kigali\": \"RW\",\n",
    "    \"oslo\": \"NO\",\n",
    "    \"zurich\": \"CH\",\n",
    "    \"san josé\": \"CR\",\n",
    "    \"auckland\": \"NZ\",\n",
    "    \"sofia\": \"BG\",\n",
    "    \"belgrade\": \"RS\",\n",
    "    \"montevideo\": \"UY\",\n",
    "    \"calgary\": \"CA\",\n",
    "    \"distrito de panama\": \"PA\",\n",
    "    \"vienna\": \"AT\",\n",
    "    \"dublin\": \"IE\",\n",
    "    \"perth\": \"AU\",\n",
    "    \"bucharest\": \"RO\",\n",
    "    \"brussels\": \"BE\",\n",
    "    \"la paz\": \"BO\",\n",
    "    \"baku\": \"AZ\",\n",
    "    \"doha\": \"QA\",\n",
    "    \"beirut\": \"LB\",\n",
    "    \"amsterdam\": \"NL\",\n",
    "    \"brisbane\": \"AU\",\n",
    "    \"vancouver\": \"CA\",\n",
    "    \"ciudad de túnez\": \"TN\",\n",
    "    \"caracas\": \"VE\",\n",
    "    \"budapest\": \"HU\",\n",
    "    \"tashkent\": \"UZ\",\n",
    "    \"kyiv\": \"UA\",\n",
    "    \"warsaw\": \"PL\",\n",
    "    \"quito\": \"EC\",\n",
    "    \"dakar\": \"SN\",\n",
    "    \"lusaka\": \"ZM\",\n",
    "    \"dubai\": \"AE\",\n",
    "    \"algiers\": \"DZ\",\n",
    "    \"casablanca\": \"MA\",\n",
    "    \"kampala\": \"UG\",\n",
    "    \"medellín\": \"CO\",\n",
    "    \"berlin\": \"DE\",\n",
    "    \"rome\": \"IT\",\n",
    "    \"montreal\": \"CA\",\n",
    "    \"amman\": \"JO\",\n",
    "    \"san francisco\": \"US\",\n",
    "    \"city of cape town\": \"ZA\",\n",
    "    \"boston\": \"US\",\n",
    "    \"accra\": \"GH\",\n",
    "    \"monterrey\": \"MX\",\n",
    "    \"nairobi\": \"KE\",\n",
    "    \"melbourne\": \"AU\",\n",
    "    \"guadalajara\": \"MX\",\n",
    "    \"sydney\": \"AU\",\n",
    "    \"alexandria\": \"EG\",\n",
    "    \"colombo\": \"LK\",\n",
    "    \"yangon\": \"MM\",\n",
    "    \"singapore\": \"SG\",\n",
    "    \"miami\": \"US\",\n",
    "    \"toronto\": \"CA\",\n",
    "    \"madrid\": \"ES\",\n",
    "    \"santiago\": \"CL\",\n",
    "    \"houston\": \"US\",\n",
    "    \"riyadh governorate\": \"SA\",\n",
    "    \"baghdad\": \"IQ\",\n",
    "    \"dar es salaam\": \"TZ\",\n",
    "    \"kuala lumpur\": \"MY\",\n",
    "    \"chicago\": \"US\",\n",
    "    \"tehran\": \"IR\",\n",
    "    \"bangkok\": \"TH\",\n",
    "    \"provincia de lima\": \"PE\",\n",
    "    \"paris\": \"FR\",\n",
    "    \"bogotá\": \"CO\",\n",
    "    \"chennai\": \"IN\",\n",
    "    \"ho chi minh city\": \"VN\",\n",
    "    \"manila\": \"PH\",\n",
    "    \"london\": \"GB\",\n",
    "    \"buenos aires\": \"AR\",\n",
    "    \"istanbul\": \"TR\",\n",
    "    \"lagos\": \"NG\",\n",
    "    \"karachi\": \"PK\",\n",
    "    \"moscow\": \"RU\",\n",
    "    \"los angeles\": \"US\",\n",
    "    \"new york\": \"US\",\n",
    "    \"mumbai\": \"IN\",\n",
    "    \"beijing\": \"CN\",\n",
    "    \"mexico city\": \"MX\",\n",
    "    \"cairo\": \"EG\",\n",
    "    \"município de são paulo\": \"BR\",\n",
    "    \"dhaka\": \"BD\",\n",
    "    \"shanghai\": \"CN\",\n",
    "    \"delhi\": \"IN\",\n",
    "    \"jakarta\": \"ID\",\n",
    "    \"coimbra\": \"PT\",\n",
    "    \"hamburg\": \"DE\",\n",
    "    \"fortaleza\": \"BR\",\n",
    "    \"tokyo\": \"JP\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5707b5d-40cf-4302-bdae-f5e31c6763c8",
   "metadata": {},
   "source": [
    "# Section 2: Define Constants and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3a896e-7289-4b99-b467-b282b4896b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:fiona._env:/Users/Matheus/Library/CloudStorage/OneDrive-Pessoal/Estudos/2 - Pesquisas/3 - CITTA/1 - RAMCCAV/3 - Assessment of Physical Road Infrastructure/Codigos/input_data/transport_agency.geojson: No such file or directory\n",
      "WARNING:root:Transport agency data file '/Users/Matheus/Library/CloudStorage/OneDrive-Pessoal/Estudos/2 - Pesquisas/3 - CITTA/1 - RAMCCAV/3 - Assessment of Physical Road Infrastructure/Codigos/input_data/transport_agency.geojson' not found. Proceeding without it. Error: /Users/Matheus/Library/CloudStorage/OneDrive-Pessoal/Estudos/2 - Pesquisas/3 - CITTA/1 - RAMCCAV/3 - Assessment of Physical Road Infrastructure/Codigos/input_data/transport_agency.geojson: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "notebook_dir = os.path.abspath('')\n",
    "PROJECT_ROOT = os.path.dirname(notebook_dir)\n",
    "INPUT_DATA_DIR = os.path.join(PROJECT_ROOT, \"input_data\")\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "\n",
    "dotenv_path = os.path.join(PROJECT_ROOT, '.env')\n",
    "load_dotenv()\n",
    "\n",
    "MLY_ACCESS_TOKEN = os.getenv(\"MLY_ACCESS_TOKEN\")\n",
    "\n",
    "if not MLY_ACCESS_TOKEN:\n",
    "    raise ValueError(\"MLY_ACCESS_TOKEN not found! Please create a .env file with your token.\")\n",
    "\n",
    "mly.set_access_token(MLY_ACCESS_TOKEN)\n",
    "\n",
    "UTM_CRS = 'EPSG:3857'\n",
    "BUFFER_DISTANCE = 10\n",
    "OSM_TAG_COLS = [f'osm_tag_{i}' for i in range(1, 6)]\n",
    "\n",
    "POINTS_CSV = os.path.join(INPUT_DATA_DIR, \"points.csv\")\n",
    "TRAFFIC_SIGNS_CSV = os.path.join(INPUT_DATA_DIR, \"traffic_signs.csv\")\n",
    "TRANSPORT_AGENCY_DATA = os.path.join(INPUT_DATA_DIR, \"transport_agency.geojson\")\n",
    "\n",
    "# --- Initialize and Load Transport Agency Data ---\n",
    "transport_agency_osm_df = None\n",
    "transport_agency_data_exists = False\n",
    "\n",
    "try:\n",
    "    transport_agency_osm_df = gpd.read_file(TRANSPORT_AGENCY_DATA)\n",
    "    if transport_agency_osm_df.crs != UTM_CRS:\n",
    "        transport_agency_osm_df = transport_agency_osm_df.to_crs(UTM_CRS)\n",
    "    transport_agency_osm_df.geometry = transport_agency_osm_df.geometry.buffer(0)\n",
    "    transport_agency_data_exists = True\n",
    "except (FileNotFoundError, DriverError) as e:\n",
    "    logging.warning(f\"Transport agency data file '{TRANSPORT_AGENCY_DATA}' not found. Proceeding without it. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970d5a2-ac64-4f75-b321-86e4459bd02c",
   "metadata": {},
   "source": [
    "# Section 3: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbaa467d-cd6a-44b7-b851-64de9d10fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_csv(filepath, osm_tag_cols=OSM_TAG_COLS):\n",
    "    \"\"\"Loads a CSV, handles NaN in OSM tag columns, and converts them to strings.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=';')\n",
    "        for col in osm_tag_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).replace('nan', '')\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found. Creating an empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=osm_tag_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfa06e7-39c6-4d3e-8977-22594b45373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mappings():\n",
    "    # Load global Mapillary -> OSM mappings (points.csv and traffic_signs.csv)\n",
    "    mapillary_points_df = load_and_preprocess_csv(POINTS_CSV)\n",
    "    mapillary_traffic_signs_df = load_and_preprocess_csv(TRAFFIC_SIGNS_CSV)\n",
    "\n",
    "    # Concatenate and handle NaNs\n",
    "    mapillary_osm_mapping = pd.concat([mapillary_points_df, mapillary_traffic_signs_df], ignore_index=True).dropna(subset=OSM_TAG_COLS, how='all')\n",
    "    mapillary_osm_mapping['mapillary_feature'] = mapillary_osm_mapping['mapillary_feature'].astype(str)\n",
    "    mapillary_osm_mapping = mapillary_osm_mapping.fillna('')  # Replace any remaining NaN with empty string\n",
    "\n",
    "    # Load country-specific mappings\n",
    "    country_mappings = {}\n",
    "    country_signs_path_pattern = os.path.join(INPUT_DATA_DIR, \"countries_signs\", \"*_traffic_signs.csv\")\n",
    "    for filename in glob.glob(country_signs_path_pattern):\n",
    "        country_code = os.path.basename(filename).split(\"_\")[0]\n",
    "        try:\n",
    "            # Specify the delimiter as a semicolon!\n",
    "            country_df = pd.read_csv(filename, encoding='utf-8', sep=';')\n",
    "            country_mappings[country_code] = country_df\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                # If utf-8 fails, try latin-1\n",
    "                country_df = pd.read_csv(filename, encoding='latin-1', sep=';')\n",
    "                country_mappings[country_code] = country_df\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "                continue\n",
    "        except Exception as e: #Catch other exceptions\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return mapillary_osm_mapping, country_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b780932-1bc1-4baa-8528-a03b69619f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- sanitize_filename function ---\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"Sanitizes a string for use as a filename.\"\"\"\n",
    "    name = name.split(',')[0].strip()  # Keep only the part before the first comma\n",
    "    name = name.lower()\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('ascii')\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name).strip()\n",
    "    name = re.sub(r'[-\\s]+', '_', name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55b61f03-a458-4774-a70e-1917116f01ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create_city_directories function ---\n",
    "def create_city_directories(base_dir, city_id):\n",
    "    \"\"\"Creates the directory structure for a city.\"\"\"\n",
    "    city_dir = os.path.join(base_dir, city_id)\n",
    "    os.makedirs(city_dir, exist_ok=True)  # Create city directory\n",
    "\n",
    "    for subdir in [\"raw\", \"OSM\", \"Mapillary\", \"missing\", \"combined\"]:\n",
    "        os.makedirs(os.path.join(city_dir, subdir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c101b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lists_to_strings(gdf):\n",
    "    \"\"\"Converts list-type columns in a GeoDataFrame to strings. Prints warnings for unsupported data types.\"\"\"\n",
    "    for col in gdf.columns:\n",
    "        if any(isinstance(val, list) for val in gdf[col]):\n",
    "            print(f\"Converting list-type column '{col}' to string.\")\n",
    "            gdf[col] = gdf[col].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else x)\n",
    "        elif any(isinstance(val, (dict, set)) for val in gdf[col]):\n",
    "            print(f\"Warning: Column '{col}' contains unsupported data types (dict/set). Skipping.\")\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab68719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema(gdf):\n",
    "    \"\"\"\n",
    "    Creates a Fiona schema from a GeoDataFrame.  Handles empty GeoDataFrames\n",
    "    correctly and dynamically determines geometry type.\n",
    "    \"\"\"\n",
    "    if gdf.empty:\n",
    "        return {\n",
    "            'geometry': 'Point',\n",
    "            'properties': {'id': 'int'}\n",
    "        }\n",
    "\n",
    "    # Get GeoDataFrame's geometry type.\n",
    "    # Correctly check for empty geometry series\n",
    "\n",
    "    try:\n",
    "        geom_type = gdf.geometry.geom_type[0]\n",
    "    except (KeyError, IndexError, AttributeError): #handle errors when accessing geom_type\n",
    "        geom_type = 'Point' # Default\n",
    "\n",
    "    schema = {\n",
    "        'geometry': geom_type,\n",
    "        'properties': OrderedDict()  # Use OrderedDict to preserve column order\n",
    "    }\n",
    "    for col_name, col_type in gdf.dtypes.items():\n",
    "        if col_name == 'geometry':\n",
    "            continue  # Geometry is handled separately\n",
    "\n",
    "        # Convert pandas/numpy types to Fiona types\n",
    "        if col_type == 'object':\n",
    "            fiona_type = 'str'\n",
    "        elif col_type == 'int64':\n",
    "            fiona_type = 'int'\n",
    "        elif col_type == 'float64':\n",
    "            fiona_type = 'float'\n",
    "        elif col_type == 'bool':\n",
    "            fiona_type = 'bool'\n",
    "        else:\n",
    "            fiona_type = 'str'  # Default to string for unknown types.\n",
    "            print(f\"Warning: Unknown column type {col_type} for column {col_name}.  Using 'str'.\")\n",
    "\n",
    "        schema['properties'][col_name] = fiona_type\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7834bc-d630-4ca5-a670-51b52dafe652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(processed_filepath, raw_filepath, download_func, *download_args):\n",
    "    \"\"\"\n",
    "    Attempts to load data with prioritized reading (processed, raw, download).\n",
    "    Saves an empty file if download returns no features.\n",
    "    Renames 'index_left' and 'index_right' columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Try to read processed data\n",
    "        logging.info(f\"Attempting to read processed data from: {processed_filepath}\")\n",
    "        data = gpd.read_file(processed_filepath, driver='GeoJSON')\n",
    "        # Rename columns if they exist\n",
    "        if 'index_left' in data.columns:\n",
    "            data = data.rename(columns={'index_left': 'orig_index_left'})\n",
    "        if 'index_right' in data.columns:\n",
    "            data = data.rename(columns={'index_right': 'orig_index_right'})\n",
    "        return data\n",
    "    except (FileNotFoundError, DriverError):\n",
    "        logging.warning(f\"Processed data file not found or unreadable: {processed_filepath}\")\n",
    "        try:\n",
    "            # 2. Try to read raw data\n",
    "            logging.info(f\"Attempting to read raw data from: {raw_filepath}\")\n",
    "            data = gpd.read_file(raw_filepath, driver='GeoJSON')\n",
    "            # Rename columns if they exist\n",
    "            if 'index_left' in data.columns:\n",
    "                data = data.rename(columns={'index_left': 'orig_index_left'})\n",
    "            if 'index_right' in data.columns:\n",
    "                data = data.rename(columns={'index_right': 'orig_index_right'})\n",
    "\n",
    "            return data\n",
    "        except (FileNotFoundError, DriverError):\n",
    "            logging.warning(f\"Raw data file not found or unreadable: {raw_filepath}\")\n",
    "            try:\n",
    "                # 3. Download data\n",
    "                logging.info(f\"Downloading data...\")\n",
    "                data = download_func(*download_args)\n",
    "\n",
    "                if not data.empty:\n",
    "                    # Save downloaded data to raw file\n",
    "                    logging.info(f\"Saving {len(data)} downloaded raw features to: {raw_filepath}\") # Log count\n",
    "                    # Use a copy to avoid modifying original 'data' before list conversion\n",
    "                    data_to_save_raw = convert_lists_to_strings(data.copy())\n",
    "                    try:\n",
    "                         # Let GeoPandas/Fiona infer schema for raw GeoJSON, which can handle mixed types better.\n",
    "                         # Do NOT provide explicit schema here for non-empty raw data.\n",
    "                         data_to_save_raw.to_file(raw_filepath, driver=\"GeoJSON\")\n",
    "                         logging.info(f\"Successfully saved raw data to: {raw_filepath}\")\n",
    "                    except Exception as save_err:\n",
    "                         # Log the error, potentially including the geometry types present\n",
    "                         geom_types_present = data_to_save_raw.geom_type.unique() if not data_to_save_raw.empty else []\n",
    "                         logging.error(f\"!!! Failed to save raw data to {raw_filepath}. Geometry types present: {geom_types_present}. Error: {save_err}\")\n",
    "                         # Decide if you want to return empty or raise error\n",
    "                         return gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                else:\n",
    "                    # Save an EMPTY GeoDataFrame - Here it is needed a schema\n",
    "                    logging.warning(f\"No features found during download. Saving empty raw file to: {raw_filepath}\")\n",
    "                    data = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS) # Use data here (which is now empty GDF)\n",
    "                    try:\n",
    "                        # Create schema for empty GDF (required by Fiona)\n",
    "                        schema = create_schema(data)\n",
    "                        data.to_file(raw_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                        logging.info(f\"Successfully saved empty raw file to: {raw_filepath}\")\n",
    "                    except Exception as save_err:\n",
    "                         logging.error(f\"!!! Failed to save empty raw data file to {raw_filepath}. Error: {save_err}\")\n",
    "                         # Decide if you want to return empty or raise error\n",
    "                         return gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "\n",
    "                #Rename columns:\n",
    "                if 'index_left' in data.columns:\n",
    "                    data = data.rename(columns={'index_left': 'orig_index_left'})\n",
    "                if 'index_right' in data.columns:\n",
    "                    data = data.rename(columns={'index_right': 'orig_index_right'})\n",
    "\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during download or raw save: {e}\")\n",
    "                return gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)  # Return empty GeoDataFrame on failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6c215ff-d874-4e9a-abea-c466134b08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_osm_data(tags, polygon, crs=UTM_CRS):\n",
    "    \"\"\"Downloads OSM data based on tags and polygon, projects to UTM, and preprocesses.\"\"\"\n",
    "    osm_data = gpd.GeoDataFrame()\n",
    "    for tag in tags:\n",
    "        if \"=\" in tag:\n",
    "            key, value = tag.split(\"=\")\n",
    "            try:\n",
    "                osm_data_part = ox.geometries_from_polygon(polygon, tags={key: value})\n",
    "                osm_data = pd.concat([osm_data, osm_data_part])\n",
    "            except ox._errors.EmptyOverpassResponse:\n",
    "                print(f\"Warning: No OSM data found for tag {tag} in the specified area.\")\n",
    "                continue  # Skip to the next tag\n",
    "\n",
    "    if osm_data.empty:\n",
    "        print(\"Warning: No OSM data found for any of the specified tags.\")\n",
    "        return gpd.GeoDataFrame(geometry=[], crs=crs)  # Return empty but valid GeoDataFrame\n",
    "\n",
    "    osm_data = osm_data.to_crs(crs)\n",
    "    osm_data = osm_data.reset_index(drop=True)\n",
    "    return osm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a83f9c36-2acb-4ec1-9c7d-3bd587be9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mapillary_data(bbox, filter_values, country_code, country_mappings, existed_at=None, existed_before=None, crs=UTM_CRS):\n",
    "    \"\"\"\n",
    "    Downloads Mapillary data and filters it locally.\n",
    "    \"\"\"\n",
    "    print(f\"download_mapillary_data called with filter_values: {filter_values}\")\n",
    "    \n",
    "    # --- Prepare Temporal Filters ---\n",
    "    temporal_filters = {}\n",
    "    if existed_at:\n",
    "        temporal_filters['existed_at'] = existed_at\n",
    "    if existed_before:\n",
    "        temporal_filters['existed_before'] = existed_before\n",
    "\n",
    "    # --- Download ALL traffic signs and points ---\n",
    "    try:\n",
    "        print(f\"Calling Mapillary API with bbox: {bbox}\")\n",
    "        data_traffic_signs = json.loads(\n",
    "            mly.traffic_signs_in_bbox(bbox, **temporal_filters)  # NO filter_values here\n",
    "        )\n",
    "        data_points = json.loads(\n",
    "            mly.map_feature_points_in_bbox(bbox, **temporal_filters)  # NO filter_values here\n",
    "        )\n",
    "        features = data_traffic_signs.get('features', []) + data_points.get('features', [])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Mapillary API calls: {e}\")\n",
    "        return gpd.GeoDataFrame(crs=crs)\n",
    "\n",
    "    if not features:\n",
    "        print(\"No features returned from Mapillary API calls.\")\n",
    "        return gpd.GeoDataFrame(crs=crs)\n",
    "\n",
    "    mapillary_data_gdf = gpd.GeoDataFrame.from_features(features, crs='EPSG:4326')\n",
    "\n",
    "    # --- Deduplicate Columns ---\n",
    "    if not mapillary_data_gdf.empty:\n",
    "        new_cols = []\n",
    "        seen_cols = set()\n",
    "        for col in mapillary_data_gdf.columns:\n",
    "            if col in seen_cols:\n",
    "                i = 1\n",
    "                new_col = f\"{col}_{i}\"\n",
    "                while new_col in seen_cols:\n",
    "                    i += 1\n",
    "                    new_col = f\"{col}_{i}\"\n",
    "                new_cols.append(new_col)\n",
    "            else:\n",
    "                new_cols.append(col)\n",
    "            seen_cols.add(new_cols[-1])\n",
    "        mapillary_data_gdf.columns = new_cols\n",
    "        mapillary_data_gdf = mapillary_data_gdf.to_crs(crs)\n",
    "\n",
    "        # --- *NOW* Filter the DataFrame ---\n",
    "        if 'value' in mapillary_data_gdf.columns:  # Check if 'value' exists\n",
    "             mapillary_data_gdf = mapillary_data_gdf[mapillary_data_gdf['value'].isin(filter_values)]\n",
    "        else:\n",
    "             return gpd.GeoDataFrame(crs=crs) #Return empty if value column does not exist\n",
    "\n",
    "\n",
    "    return mapillary_data_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baff60b5-7a60-4251-831f-ece224c327ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mapillary_data(mapillary_data_gdf, polygon):\n",
    "    \"\"\"\n",
    "    Filters Mapillary data to retain only items within the original search area (polygon).\n",
    "    \"\"\"\n",
    "    if not mapillary_data_gdf.empty:\n",
    "        # Ensure both GeoDataFrames are in the same CRS\n",
    "        mapillary_data_gdf = mapillary_data_gdf.to_crs(polygon.crs)\n",
    "\n",
    "        # --- Rename columns BEFORE sjoin ---\n",
    "        if 'index_left' in mapillary_data_gdf.columns:\n",
    "            mapillary_data_gdf = mapillary_data_gdf.rename(columns={'index_left': 'orig_index_left'})\n",
    "        if 'index_right' in mapillary_data_gdf.columns:\n",
    "            mapillary_data_gdf = mapillary_data_gdf.rename(columns={'index_right': 'orig_index_right'})\n",
    "        if 'index_left' in polygon.columns:\n",
    "            polygon = polygon.rename(columns={'index_left': 'orig_index_left'})\n",
    "        if 'index_right' in polygon.columns:\n",
    "            polygon = polygon.rename(columns={'index_right': 'orig_index_right'})\n",
    "\n",
    "        # Perform spatial join to filter items within the polygon\n",
    "        filtered_data = gpd.sjoin(mapillary_data_gdf, polygon, predicate=\"within\", how = 'inner')\n",
    "        return filtered_data\n",
    "    return gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6e30ba-9342-446f-bd2c-ad431654daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_overlap_2dfs(df2, df3, properties_to_keep=None, buffer_distance=BUFFER_DISTANCE):\n",
    "    \"\"\"\n",
    "    Analyzes overlap (2 DataFrames), with detailed debugging for 'highway'.\n",
    "    \"\"\"\n",
    "    def combine_dataframes(df_base, df_add, properties_to_keep):\n",
    "\n",
    "        # --- Explicitly handle 'highway' ---\n",
    "        df_add['highway'] = ''\n",
    "        for osm_tag_col in OSM_TAG_COLS:\n",
    "            if osm_tag_col in df_add.columns:\n",
    "                df_add['highway'] = np.where(\n",
    "                    (df_add[osm_tag_col].notna()) & (df_add[osm_tag_col].str.startswith('highway=')),\n",
    "                    df_add[osm_tag_col].str.replace('highway=', '', 1),\n",
    "                    df_add['highway']\n",
    "                )\n",
    "    \n",
    "        exclude_cols = df_add.columns.intersection(['index_left', 'index_right'])\n",
    "    \n",
    "        # --- Prepare df_add_processed ---\n",
    "        if properties_to_keep is None:\n",
    "            cols_to_add = df_add.columns.difference(df_base.columns).union(['geometry'])\n",
    "            cols_to_add = cols_to_add.difference(exclude_cols)\n",
    "             # Ensure geometry column is present if df_add is a GeoDataFrame\n",
    "            if 'geometry' not in cols_to_add and isinstance(df_add, gpd.GeoDataFrame) and 'geometry' in df_add.columns:\n",
    "                cols_to_add = cols_to_add.union(['geometry'])\n",
    "            df_add_processed = df_add[list(cols_to_add)].copy() # Use list for explicit column order\n",
    "        else:\n",
    "            keep_cols = [col for col in properties_to_keep if col in df_add.columns]\n",
    "            # Ensure geometry column is present if df_add is a GeoDataFrame\n",
    "            if 'geometry' not in keep_cols and isinstance(df_add, gpd.GeoDataFrame) and 'geometry' in df_add.columns:\n",
    "                keep_cols.append('geometry')\n",
    "            df_add_processed = df_add[keep_cols].copy()\n",
    "            df_add_processed = df_add_processed.drop(columns=exclude_cols.intersection(df_add_processed.columns), errors='ignore')\n",
    "    \n",
    "        # --- Ensure Both are GeoDataFrames with Geometry (if possible) ---\n",
    "        base_is_gdf = isinstance(df_base, gpd.GeoDataFrame) and 'geometry' in df_base.columns\n",
    "        add_is_gdf = isinstance(df_add_processed, gpd.GeoDataFrame) and 'geometry' in df_add_processed.columns\n",
    "    \n",
    "        if base_is_gdf and add_is_gdf:\n",
    "            base_crs = df_base.crs # Store CRS\n",
    "    \n",
    "            # --- Align CRS BEFORE concatenation ---\n",
    "            if df_base.crs != df_add_processed.crs:\n",
    "                print(f\"Aligning df_add_processed CRS from {df_add_processed.crs} to {base_crs}\")\n",
    "                df_add_processed = df_add_processed.to_crs(base_crs)\n",
    "    \n",
    "            # --- Concatenate ---\n",
    "            try:\n",
    "                # Use sort=False to prevent potential column reordering issues\n",
    "                combined_df = pd.concat([df_base, df_add_processed], ignore_index=True, sort=False)\n",
    "    \n",
    "                # --- Create GeoDataFrame after concat ---\n",
    "                print(\"Creating GeoDataFrame after concat...\")\n",
    "                # Ensure the geometry column exists before creating GeoDataFrame\n",
    "                if 'geometry' in combined_df.columns:\n",
    "                    combined_gdf = gpd.GeoDataFrame(combined_df, geometry='geometry', crs=base_crs) # Use base CRS\n",
    "                    print(\"----- Exiting combine_dataframes (GeoDataFrame) -----\")\n",
    "                    return combined_gdf\n",
    "                else:\n",
    "                    print(\"Warning: Geometry column lost during concat. Returning DataFrame.\")\n",
    "                    print(\"----- Exiting combine_dataframes (DataFrame - No Geometry) -----\")\n",
    "                    return combined_df # Return as DataFrame if geometry is lost\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Error during GeoDataFrame creation after concat: {e}. Returning DataFrame.\")\n",
    "                # Fallback to regular DataFrame, ensure geometry is dropped if it caused issues\n",
    "                combined_df = pd.concat([df_base, df_add_processed], ignore_index=True, sort=False)\n",
    "                if 'geometry' in combined_df.columns:\n",
    "                     combined_df = combined_df.drop(columns=['geometry'], errors='ignore')\n",
    "                print(\"----- Exiting combine_dataframes (DataFrame - Fallback) -----\")\n",
    "                return combined_df\n",
    "    \n",
    "        # --- Fallback cases if inputs are not GDFs ---\n",
    "        else:\n",
    "             print(\"Warning: One or both inputs are not GeoDataFrames with geometry. Concatenating attributes only.\")\n",
    "             df_base_attrs = df_base.drop(columns=['geometry'], errors='ignore') if base_is_gdf else df_base\n",
    "             df_add_processed_attrs = df_add_processed.drop(columns=['geometry'], errors='ignore') if add_is_gdf else df_add_processed\n",
    "             combined_attrs = pd.concat([df_base_attrs, df_add_processed_attrs], ignore_index=True, sort=False)\n",
    "             print(\"----- Exiting combine_dataframes (DataFrame - Attrs Only) -----\")\n",
    "             return combined_attrs\n",
    "\n",
    "    for df in [df2, df3]:\n",
    "        df.drop(columns=['index_left', 'index_right'], inplace=True, errors='ignore')\n",
    "    df2 = df2.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    df3 = df3.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    df2_buffered = df2.copy()\n",
    "    df3_buffered = df3.copy()\n",
    "    df2_buffered.geometry = df2_buffered.geometry.buffer(buffer_distance)\n",
    "    df3_buffered.geometry = df3_buffered.geometry.buffer(buffer_distance)\n",
    "    overlap_2_3_buffered = gpd.sjoin(df3_buffered, df2_buffered, how=\"inner\", predicate=\"intersects\")\n",
    "    overlap_2_3 = df3.loc[overlap_2_3_buffered.index].copy()\n",
    "    overlap_2_3 = overlap_2_3.merge(overlap_2_3_buffered.drop(columns='geometry'), left_index=True, right_index=True)\n",
    "    missing_indices = df3.index.difference(overlap_2_3.index)\n",
    "    missing_2_3 = df3.loc[missing_indices].copy()\n",
    "\n",
    "    df2['source'] = 'OSM'\n",
    "    df3['source'] = 'Mapillary'\n",
    "    overlap_2_3['source'] = df3['source']\n",
    "    missing_2_3['source'] = df3['source']\n",
    "    if 'highway' in df2.columns: #Keep highway if it already exists\n",
    "        df2['highway'] = df2['highway']\n",
    "\n",
    "\n",
    "    combined_2_3 = combine_dataframes(df2, missing_2_3, properties_to_keep)\n",
    "    print(\"--- combined_2_3 (first 5 rows):\\n\", combined_2_3.head())  # Debug print\n",
    "    print(\"--- combined_2_3 dtypes:\\n\", combined_2_3.dtypes)  # Debug print\n",
    "    return combined_2_3, overlap_2_3, missing_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00a7de58-bd8e-400f-a0bc-2b35e0385706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_overlap_3dfs(df1, df2, df3, properties_to_keep=None, buffer_distance=BUFFER_DISTANCE):\n",
    "    \"\"\"\n",
    "    Analyzes overlap (3 DataFrames), with detailed debugging for 'highway'.\n",
    "    \"\"\"\n",
    "    def combine_dataframes(df_base, df_add, properties_to_keep):\n",
    "\n",
    "        # --- Explicitly handle 'highway' ---\n",
    "        df_add['highway'] = ''\n",
    "        for osm_tag_col in OSM_TAG_COLS:\n",
    "            if osm_tag_col in df_add.columns:\n",
    "                df_add['highway'] = np.where(\n",
    "                    (df_add[osm_tag_col].notna()) & (df_add[osm_tag_col].str.startswith('highway=')),\n",
    "                    df_add[osm_tag_col].str.replace('highway=', '', 1),\n",
    "                    df_add['highway']\n",
    "                )\n",
    "    \n",
    "        exclude_cols = df_add.columns.intersection(['index_left', 'index_right'])\n",
    "    \n",
    "        # --- Prepare df_add_processed ---\n",
    "        if properties_to_keep is None:\n",
    "            cols_to_add = df_add.columns.difference(df_base.columns).union(['geometry'])\n",
    "            cols_to_add = cols_to_add.difference(exclude_cols)\n",
    "             # Ensure geometry column is present if df_add is a GeoDataFrame\n",
    "            if 'geometry' not in cols_to_add and isinstance(df_add, gpd.GeoDataFrame) and 'geometry' in df_add.columns:\n",
    "                cols_to_add = cols_to_add.union(['geometry'])\n",
    "            df_add_processed = df_add[list(cols_to_add)].copy() # Use list for explicit column order\n",
    "        else:\n",
    "            keep_cols = [col for col in properties_to_keep if col in df_add.columns]\n",
    "            # Ensure geometry column is present if df_add is a GeoDataFrame\n",
    "            if 'geometry' not in keep_cols and isinstance(df_add, gpd.GeoDataFrame) and 'geometry' in df_add.columns:\n",
    "                keep_cols.append('geometry')\n",
    "            df_add_processed = df_add[keep_cols].copy()\n",
    "            df_add_processed = df_add_processed.drop(columns=exclude_cols.intersection(df_add_processed.columns), errors='ignore')\n",
    "    \n",
    "        # --- Ensure Both are GeoDataFrames with Geometry (if possible) ---\n",
    "        base_is_gdf = isinstance(df_base, gpd.GeoDataFrame) and 'geometry' in df_base.columns\n",
    "        add_is_gdf = isinstance(df_add_processed, gpd.GeoDataFrame) and 'geometry' in df_add_processed.columns\n",
    "    \n",
    "        if base_is_gdf and add_is_gdf:\n",
    "            base_crs = df_base.crs # Store CRS\n",
    "    \n",
    "            # --- Align CRS BEFORE concatenation ---\n",
    "            if df_base.crs != df_add_processed.crs:\n",
    "                print(f\"Aligning df_add_processed CRS from {df_add_processed.crs} to {base_crs}\")\n",
    "                df_add_processed = df_add_processed.to_crs(base_crs)\n",
    "    \n",
    "            # --- Concatenate ---\n",
    "            try:\n",
    "                # Use sort=False to prevent potential column reordering issues\n",
    "                combined_df = pd.concat([df_base, df_add_processed], ignore_index=True, sort=False)\n",
    "    \n",
    "                # --- Create GeoDataFrame after concat ---\n",
    "                print(\"Creating GeoDataFrame after concat...\")\n",
    "                # Ensure the geometry column exists before creating GeoDataFrame\n",
    "                if 'geometry' in combined_df.columns:\n",
    "                    combined_gdf = gpd.GeoDataFrame(combined_df, geometry='geometry', crs=base_crs) # Use base CRS\n",
    "                    print(\"----- Exiting combine_dataframes (GeoDataFrame) -----\")\n",
    "                    return combined_gdf\n",
    "                else:\n",
    "                    print(\"Warning: Geometry column lost during concat. Returning DataFrame.\")\n",
    "                    print(\"----- Exiting combine_dataframes (DataFrame - No Geometry) -----\")\n",
    "                    return combined_df # Return as DataFrame if geometry is lost\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Error during GeoDataFrame creation after concat: {e}. Returning DataFrame.\")\n",
    "                # Fallback to regular DataFrame, ensure geometry is dropped if it caused issues\n",
    "                combined_df = pd.concat([df_base, df_add_processed], ignore_index=True, sort=False)\n",
    "                if 'geometry' in combined_df.columns:\n",
    "                     combined_df = combined_df.drop(columns=['geometry'], errors='ignore')\n",
    "                print(\"----- Exiting combine_dataframes (DataFrame - Fallback) -----\")\n",
    "                return combined_df\n",
    "    \n",
    "        # --- Fallback cases if inputs are not GDFs ---\n",
    "        else:\n",
    "             print(\"Warning: One or both inputs are not GeoDataFrames with geometry. Concatenating attributes only.\")\n",
    "             df_base_attrs = df_base.drop(columns=['geometry'], errors='ignore') if base_is_gdf else df_base\n",
    "             df_add_processed_attrs = df_add_processed.drop(columns=['geometry'], errors='ignore') if add_is_gdf else df_add_processed\n",
    "             combined_attrs = pd.concat([df_base_attrs, df_add_processed_attrs], ignore_index=True, sort=False)\n",
    "             print(\"----- Exiting combine_dataframes (DataFrame - Attrs Only) -----\")\n",
    "             return combined_attrs\n",
    "\n",
    "    for df in [df1, df2, df3]:\n",
    "        df.drop(columns=['index_left', 'index_right'], inplace=True, errors='ignore')\n",
    "\n",
    "    df1 = df1.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    df2 = df2.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    df3 = df3.drop_duplicates(subset='geometry').reset_index(drop=True) #Add this\n",
    "\n",
    "    df1_buffered = df1.copy()\n",
    "    df2_buffered = df2.copy()\n",
    "    df3_buffered = df3.copy()\n",
    "    df1_buffered.geometry = df1_buffered.geometry.buffer(buffer_distance)\n",
    "    df2_buffered.geometry = df2_buffered.geometry.buffer(buffer_distance)\n",
    "    df3_buffered.geometry = df3_buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "    overlap_1_2_buffered = gpd.sjoin(df2_buffered, df1_buffered, how=\"inner\", predicate=\"intersects\")\n",
    "    overlap_1_2 = df2.loc[overlap_1_2_buffered.index].copy()\n",
    "    overlap_1_2 = overlap_1_2.merge(overlap_1_2_buffered.drop(columns='geometry'), left_index=True, right_index=True)\n",
    "    missing_indices_1_2 = df2.index.difference(overlap_1_2.index)\n",
    "    missing_1_2 = df2.loc[missing_indices_1_2].copy()\n",
    "\n",
    "    df1['source'] = 'transit_agency'\n",
    "    df2['source'] = 'OSM'\n",
    "    df3['source'] = 'Mapillary'\n",
    "    overlap_1_2['source'] = df2['source']\n",
    "    missing_1_2['source'] = df2['source']\n",
    "\n",
    "    # --- Add 'highway' from df2 if it exists ---\n",
    "    if 'highway' in df2.columns:\n",
    "        df2['highway'] = df2['highway']\n",
    "    combined_1_2 = combine_dataframes(df1, missing_1_2, properties_to_keep)\n",
    "\n",
    "    overlap_2_3_buffered = gpd.sjoin(df3_buffered, combined_1_2, how=\"inner\", predicate=\"intersects\")\n",
    "    overlap_2_3 = df3.loc[overlap_2_3_buffered.index].copy()\n",
    "    overlap_2_3 = overlap_2_3.merge(overlap_2_3_buffered.drop(columns='geometry'), left_index=True, right_index=True)\n",
    "    missing_indices_2_3 = df3.index.difference(overlap_2_3.index)\n",
    "    missing_2_3 = df3.loc[missing_indices_2_3].copy()\n",
    "\n",
    "    overlap_2_3['source'] = df3['source']\n",
    "    missing_2_3['source'] = df3['source']\n",
    "\n",
    "    combined_2_3 = combine_dataframes(combined_1_2, missing_2_3, properties_to_keep)\n",
    "    print(\"--- combined_2_3 (first 5 rows):\\n\", combined_2_3.head())  # Debug print\n",
    "    print(\"--- combined_2_3 dtypes:\\n\", combined_2_3.dtypes)\n",
    "    return combined_2_3, combined_1_2, overlap_1_2, missing_1_2, overlap_2_3, missing_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0997779d-53d7-4160-814d-625a08d22338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_osm_tags_to_kv(gdf, osm_tag_cols=OSM_TAG_COLS):\n",
    "    \"\"\"Converts osm_tag_N columns to standard OSM key-value pairs, AND handles country_tag.\"\"\"\n",
    "\n",
    "    original_crs = getattr(gdf, 'crs', UTM_CRS) # Get CRS early\n",
    "\n",
    "    if not isinstance(gdf, gpd.GeoDataFrame) or gdf.empty:\n",
    "        # logging.debug(\"Input (Geo)DataFrame to convert_osm_tags_to_kv is empty. Returning empty.\")\n",
    "        return gpd.GeoDataFrame(geometry=[], crs=original_crs), [] # Return empty GDF with CRS\n",
    "\n",
    "    gdf_copy = gdf.copy()\n",
    "    if 'mapillary_feature' in gdf_copy.columns:\n",
    "        gdf_copy = gdf_copy.drop(columns=['mapillary_feature'], errors='ignore')\n",
    "\n",
    "    osm_data = {}\n",
    "    original_indices = gdf_copy.index\n",
    "\n",
    "    # --- Preserve Existing Tags (More Robust) ---\n",
    "    essential_cols = ['geometry', 'source'] # Keep these separate\n",
    "    # Identify potential existing tag columns (anything not essential or osm_tag/country_tag)\n",
    "    # Explicitly exclude 'nodes' which is often a list from OSMnx\n",
    "    other_cols = gdf_copy.columns.difference(essential_cols + osm_tag_cols + ['country_tag', 'nodes']) # Added 'nodes' exclusion\n",
    "\n",
    "    for index, row in gdf_copy.iterrows():\n",
    "        osm_data[index] = {}\n",
    "        # Add existing non-tag columns as potential tags\n",
    "        for col in other_cols:\n",
    "            # Check if it's likely an attribute, not internal GeoPandas stuff\n",
    "            if col not in ['level_0', 'index_right', 'orig_index_left', 'orig_index_right']: # Keep internal GPD excludes\n",
    "                value_at_row_col = row[col] # Get value once\n",
    "\n",
    "                # Check if the value is scalar BEFORE applying pd.notna or str()\n",
    "                if np.isscalar(value_at_row_col):\n",
    "                    # Now safe to treat as scalar\n",
    "                    if pd.notna(value_at_row_col) and str(value_at_row_col) != '':\n",
    "                        osm_data[index][col] = str(value_at_row_col)\n",
    "                # Optional: Log if we skip a non-scalar value? Helps debugging.\n",
    "                # else:\n",
    "                #    if isinstance(value_at_row_col, (list, np.ndarray)):\n",
    "                #       logging.debug(f\"Skipping non-scalar (list/array) value in column '{col}' for index {index}\")\n",
    "                #    else:\n",
    "                #       logging.debug(f\"Skipping non-scalar type '{type(value_at_row_col)}' in column '{col}' for index {index}\")\n",
    "\n",
    "    # --- Process osm_tag_N columns ---\n",
    "    for col in osm_tag_cols:\n",
    "        if col in gdf_copy.columns:\n",
    "            for index, row in gdf_copy.iterrows():\n",
    "                if index not in osm_data: osm_data[index] = {}\n",
    "                tag_value = row[col]\n",
    "                # Make sure tag_value is scalar here too before splitting (less likely an issue here, but good practice)\n",
    "                if np.isscalar(tag_value) and pd.notna(tag_value) and tag_value != '':\n",
    "                    if isinstance(tag_value, str) and \"=\" in tag_value:\n",
    "                        key, value = tag_value.split(\"=\", 1)\n",
    "                        # Check key/value validity after split\n",
    "                        if pd.notna(key) and pd.notna(value) and key != '' and value != '':\n",
    "                            if key not in osm_data[index]:\n",
    "                                osm_data[index][key] = value\n",
    "                            # Avoid duplication if value already exists in a semi-colon list\n",
    "                            elif value not in osm_data[index][key].split(';'):\n",
    "                                osm_data[index][key] = f\"{osm_data[index][key]};{value}\"\n",
    "            # Don't drop columns here\n",
    "\n",
    "    # --- Handle country_tag ---\n",
    "    if 'country_tag' in gdf_copy.columns:\n",
    "        for index, row in gdf_copy.iterrows():\n",
    "            if index not in osm_data: osm_data[index] = {}\n",
    "            country_tag_value = row['country_tag']\n",
    "            # Ensure it's scalar and notna\n",
    "            if np.isscalar(country_tag_value) and pd.notna(country_tag_value) and country_tag_value != '':\n",
    "                # Extract value after 'traffic_sign=' if present\n",
    "                value_part = country_tag_value.split(\"traffic_sign=\", 1)[-1] # Takes last element, works if prefix exists or not\n",
    "                if value_part: # Ensure extracted part is not empty\n",
    "                   osm_data[index]['traffic_sign'] = value_part # Assign directly to 'traffic_sign' key\n",
    "        # Don't drop column here\n",
    "\n",
    "    processed_tags_df = pd.DataFrame.from_dict(osm_data, orient='index')\n",
    "\n",
    "    # --- Combine essential columns with processed tags ---\n",
    "    existing_essential_cols = [col for col in essential_cols if col in gdf_copy.columns]\n",
    "    if not existing_essential_cols and processed_tags_df.empty:\n",
    "        logging.warning(\"convert_osm_tags_to_kv: No essential columns or processed tags found.\")\n",
    "        return gpd.GeoDataFrame(geometry=[], crs=original_crs), []\n",
    "    elif not existing_essential_cols:\n",
    "        essential_df = gpd.GeoDataFrame(index=original_indices, crs=original_crs) # Create empty with index\n",
    "    else:\n",
    "         essential_df = gdf_copy.loc[original_indices, existing_essential_cols].copy()\n",
    "\n",
    "    if processed_tags_df.empty:\n",
    "        gdf_merged = essential_df # If no tags, result is just essential cols\n",
    "    else:\n",
    "        # Ensure index alignment before concatenation/merge\n",
    "        processed_tags_df = processed_tags_df.reindex(essential_df.index)\n",
    "        # Use concat which might be safer than merge for just adding columns\n",
    "        gdf_merged = pd.concat([essential_df, processed_tags_df], axis=1)\n",
    "\n",
    "\n",
    "    # --- Geometry Handling ---\n",
    "    is_geo = False\n",
    "    if 'geometry' in gdf_merged.columns:\n",
    "         # Convert to GeoDataFrame if it's not already (might be DataFrame after concat)\n",
    "         if not isinstance(gdf_merged, gpd.GeoDataFrame):\n",
    "              try:\n",
    "                  gdf_merged = gpd.GeoDataFrame(gdf_merged, geometry='geometry', crs=original_crs)\n",
    "                  is_geo = True\n",
    "              except Exception as gdf_err:\n",
    "                  logging.warning(f\"Could not create GeoDataFrame during conversion: {gdf_err}\")\n",
    "                  is_geo = False\n",
    "         else: # It was already a GeoDataFrame\n",
    "             is_geo = True\n",
    "             # Ensure CRS consistency\n",
    "             if gdf_merged.crs is None: gdf_merged = gdf_merged.set_crs(original_crs)\n",
    "             elif gdf_merged.crs != original_crs: gdf_merged = gdf_merged.to_crs(original_crs)\n",
    "             # Ensure active geometry\n",
    "             if gdf_merged.geometry.name != 'geometry': gdf_merged = gdf_merged.set_geometry('geometry')\n",
    "\n",
    "\n",
    "         # Validate and fix only if we successfully have a GeoDataFrame\n",
    "         if is_geo and not gdf_merged.geometry.is_valid.all():\n",
    "             logging.warning(\"Invalid geometries found. Attempting buffer(0)...\")\n",
    "             try:\n",
    "                 original_geom_count = len(gdf_merged)\n",
    "                 buffered_geoms = gdf_merged.geometry.buffer(0)\n",
    "                 # Filter out empty geometries resulting from buffer(0)\n",
    "                 gdf_merged = gdf_merged[~buffered_geoms.is_empty].copy() # Use copy to avoid SettingWithCopyWarning\n",
    "                 if not gdf_merged.empty:\n",
    "                      gdf_merged.geometry = gdf_merged.geometry.buffer(0) # Re-apply buffer on filtered subset if needed for validity\n",
    "                      if not gdf_merged.geometry.is_valid.all():\n",
    "                          logging.warning(\"buffer(0) failed to fix all. Dropping remaining invalid.\")\n",
    "                          gdf_merged = gdf_merged[gdf_merged.geometry.is_valid]\n",
    "                      logging.info(f\"Geometry fixing removed {original_geom_count - len(gdf_merged)} empty/invalid features.\")\n",
    "                 else:\n",
    "                      logging.warning(\"All geometries became empty after buffer(0).\")\n",
    "\n",
    "                 # Ensure CRS is maintained\n",
    "                 if not gdf_merged.empty and gdf_merged.crs != original_crs:\n",
    "                      gdf_merged = gdf_merged.set_crs(original_crs)\n",
    "\n",
    "             except Exception as e:\n",
    "                  logging.error(f\"Error during buffer(0): {e}. Attempting to keep valid only.\")\n",
    "                  gdf_merged = gdf_merged[gdf_merged.geometry.is_valid]\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    cols_to_fill = gdf_merged.columns.difference(['geometry']) if is_geo and 'geometry' in gdf_merged.columns else gdf_merged.columns\n",
    "    gdf_merged[cols_to_fill] = gdf_merged[cols_to_fill].fillna('')\n",
    "    cols_to_drop_final = list(set(osm_tag_cols + ['country_tag']).intersection(gdf_merged.columns))\n",
    "    gdf_merged = gdf_merged.drop(columns=cols_to_drop_final, errors='ignore')\n",
    "\n",
    "    # Return correct type\n",
    "    if is_geo and 'geometry' in gdf_merged.columns and not gdf_merged.empty:\n",
    "         if not isinstance(gdf_merged, gpd.GeoDataFrame): # Final check\n",
    "             try: gdf_merged = gpd.GeoDataFrame(gdf_merged, geometry='geometry', crs=original_crs)\n",
    "             except: return pd.DataFrame(gdf_merged.drop(columns=['geometry'], errors='ignore')), []\n",
    "         return gdf_merged, []\n",
    "    elif 'geometry' in gdf_merged.columns: # Attempt final conversion\n",
    "         try: return gpd.GeoDataFrame(gdf_merged, geometry='geometry', crs=original_crs), []\n",
    "         except: return pd.DataFrame(gdf_merged.drop(columns=['geometry'], errors='ignore')), []\n",
    "    else:\n",
    "        return pd.DataFrame(gdf_merged), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d10c271-ec4f-48de-a053-25b5c6827687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_from_input(user_input, city_id, manual_bounds_dir=os.path.join(INPUT_DATA_DIR, \"manual_boundaries\")):\n",
    "    \"\"\"\n",
    "    Converts user input (place name or polygon) into a GeoDataFrame and its bounding box.\n",
    "    PRIORITIZES loading a manual GeoJSON boundary file if it exists.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): The original place name string (used for fallback geocoding).\n",
    "        city_id (str): The sanitized city ID used for filename matching.\n",
    "        manual_bounds_dir (str): The directory where manual boundary files are stored.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (GeoDataFrame containing the boundary polygon, dictionary containing bbox)\n",
    "               Returns (None, None) if area cannot be determined.\n",
    "    \"\"\"\n",
    "    manual_geojson_path = os.path.join(manual_bounds_dir, f\"{city_id}.geojson\")\n",
    "    logging.info(f\"Checking for manual boundary file: {manual_geojson_path}\")\n",
    "\n",
    "    area_gdf = None\n",
    "    polygon = None\n",
    "\n",
    "    # --- Attempt 1: Load Manual GeoJSON ---\n",
    "    try:\n",
    "        if os.path.exists(manual_geojson_path):\n",
    "            area_gdf = gpd.read_file(manual_geojson_path)\n",
    "            if area_gdf.empty or 'geometry' not in area_gdf.columns:\n",
    "                raise ValueError(\"Manual GeoJSON is empty or lacks geometry column.\")\n",
    "            if len(area_gdf) > 1:\n",
    "                logging.warning(f\"Manual GeoJSON {manual_geojson_path} has multiple features. Using the first one.\")\n",
    "                area_gdf = area_gdf.iloc[[0]].copy() # Select only the first feature\n",
    "\n",
    "            # Ensure CRS is EPSG:4326 initially (like geocode_to_gdf) for consistency downstream\n",
    "            if area_gdf.crs is None:\n",
    "                 logging.warning(f\"Manual GeoJSON {manual_geojson_path} has no CRS defined. Assuming EPSG:4326.\")\n",
    "                 area_gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "            elif area_gdf.crs.to_epsg() != 4326:\n",
    "                 logging.info(f\"Projecting manual GeoJSON from {area_gdf.crs} to EPSG:4326.\")\n",
    "                 area_gdf = area_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "            # Validate geometry type (should be Polygon or MultiPolygon)\n",
    "            geom_type = area_gdf.geometry.iloc[0].geom_type\n",
    "            if not geom_type.endswith('Polygon'):\n",
    "                 raise ValueError(f\"Geometry in {manual_geojson_path} is not a Polygon/MultiPolygon ({geom_type}).\")\n",
    "\n",
    "            polygon = area_gdf.geometry.iloc[0]\n",
    "            logging.info(f\"Successfully loaded manual boundary from {manual_geojson_path}\")\n",
    "\n",
    "        else:\n",
    "             raise FileNotFoundError # Explicitly raise if file doesn't exist to trigger fallback\n",
    "\n",
    "    except (FileNotFoundError, DriverError, ValueError, Exception) as e_manual:\n",
    "        if not isinstance(e_manual, FileNotFoundError): # Log errors other than file not found\n",
    "             logging.warning(f\"Could not load or validate manual boundary file {manual_geojson_path}: {e_manual}. Falling back to geocoding.\")\n",
    "        else:\n",
    "             logging.info(f\"Manual boundary file not found. Falling back to geocoding '{user_input}'.\")\n",
    "\n",
    "        # --- Attempt 2: Fallback to Geocoding ---\n",
    "        try:\n",
    "            logging.debug(f\"Attempting ox.geocode_to_gdf for: '{user_input}'\")\n",
    "            area_gdf = ox.geocode_to_gdf(user_input)\n",
    "            if area_gdf.empty or 'geometry' not in area_gdf.columns:\n",
    "                 raise ValueError(f\"ox.geocode_to_gdf returned empty or no geometry for '{user_input}'\")\n",
    "            # OSMnx usually returns 4326, but check just in case\n",
    "            if area_gdf.crs is None: area_gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "            elif area_gdf.crs.to_epsg() != 4326: area_gdf = area_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "            polygon = area_gdf.geometry.iloc[0] # Get the first (and usually only) polygon\n",
    "            logging.info(f\"Successfully geocoded boundary for '{user_input}'\")\n",
    "\n",
    "        except Exception as e_geocode:\n",
    "            logging.error(f\"Failed to determine area boundary for '{user_input}' using both manual file and geocoding: {e_geocode}\", exc_info=True)\n",
    "            return None, None # Indicate failure\n",
    "\n",
    "    # --- Post-processing (applies to both manual and geocoded) ---\n",
    "    # Rename conflicting columns potential added by GeoPandas/OSMnx internal ops\n",
    "    if 'index_left' in area_gdf.columns:\n",
    "        area_gdf = area_gdf.rename(columns={'index_left': 'orig_index_left'})\n",
    "    if 'index_right' in area_gdf.columns:\n",
    "        area_gdf = area_gdf.rename(columns={'index_right': 'orig_index_right'})\n",
    "\n",
    "    # Extract bbox from the final polygon\n",
    "    try:\n",
    "        bbox = {\n",
    "            'west': polygon.bounds[0],\n",
    "            'south': polygon.bounds[1],\n",
    "            'east': polygon.bounds[2],\n",
    "            'north': polygon.bounds[3]\n",
    "        }\n",
    "    except Exception as e_bbox:\n",
    "         logging.error(f\"Failed to extract bounds from polygon for {city_id}: {e_bbox}\")\n",
    "         return None, None\n",
    "\n",
    "    return area_gdf, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "100e0e5e-9e5e-4bfe-91c1-2fc718d9d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_code(place_name, country_mappings):\n",
    "    \"\"\"Uses geopy to get the country code, with fallbacks, URL encoding, and place name simplification.\"\"\"\n",
    "\n",
    "    # 1. Try the dictionary first (case-insensitive lookup)\n",
    "    lookup_key = place_name.lower().strip() # Use lowercase, stripped key\n",
    "    logging.debug(f\"Attempting dictionary lookup for key: '{lookup_key}'\")\n",
    "    country_code = place_to_country.get(lookup_key) # Use the processed key\n",
    "    if country_code:\n",
    "        logging.info(f\"Found country code '{country_code}' in dictionary for '{place_name}'\")\n",
    "        return country_code\n",
    "    logging.debug(\"Not found in dictionary.\")\n",
    "\n",
    "    # 2. If not in the dictionary, try geopy\n",
    "    logging.info(f\"Attempting geopy lookup for: '{place_name}'\")\n",
    "    try:\n",
    "        # --- Simplification Loop ---\n",
    "        parts = place_name.split(',')\n",
    "        # Start with the most specific query and simplify\n",
    "        for i in range(len(parts), 0, -1):\n",
    "            simplified_place_name = ','.join(parts[:i]).strip()\n",
    "            # Skip if simplified name is empty\n",
    "            if not simplified_place_name:\n",
    "                 continue\n",
    "\n",
    "            encoded_simplified_name = urllib.parse.quote(simplified_place_name)\n",
    "            logging.debug(f\"Trying geopy with simplified name: '{simplified_place_name}' (Encoded: {encoded_simplified_name})\")\n",
    "\n",
    "            # --- Geocoding Attempts ---\n",
    "            location = None\n",
    "            try:\n",
    "                 # Attempt 1: Direct geocode\n",
    "                 logging.debug(f\"  Querying Nominatim (direct): '{simplified_place_name}'\")\n",
    "                 location = geocode(simplified_place_name, timeout=10) # Use RateLimiter wrapper, add timeout\n",
    "                 logging.debug(f\"  Nominatim direct result: {location.raw if location else 'None'}\")\n",
    "                 if location: # Check if found before proceeding\n",
    "                      address = location.raw.get('address', {})\n",
    "                      cc = address.get('country_code')\n",
    "                      if cc: logging.info(f\"  Found country code '{cc.upper()}' via direct geocode.\"); return cc.upper()\n",
    "            except Exception as e_geo1:\n",
    "                 logging.warning(f\"  Nominatim direct query failed for '{simplified_place_name}': {e_geo1}\")\n",
    "\n",
    "            # Attempt 2: Try with addressdetails=True if direct failed or didn't have code\n",
    "            if not location or not location.raw.get('address', {}).get('country_code'):\n",
    "                 try:\n",
    "                      logging.debug(f\"  Querying Nominatim (addressdetails): '{simplified_place_name}'\")\n",
    "                      location_details = geocode(simplified_place_name, addressdetails=True, timeout=10)\n",
    "                      logging.debug(f\"  Nominatim addressdetails result: {location_details.raw if location_details else 'None'}\")\n",
    "                      if location_details:\n",
    "                           address_details = location_details.raw.get('address', {})\n",
    "                           cc = address_details.get('country_code')\n",
    "                           if cc: logging.info(f\"  Found country code '{cc.upper()}' via addressdetails.\"); return cc.upper()\n",
    "                 except Exception as e_geo2:\n",
    "                      logging.warning(f\"  Nominatim addressdetails query failed for '{simplified_place_name}': {e_geo2}\")\n",
    "\n",
    "        # If loop finishes without finding code\n",
    "        print(f\"Warning: Could not find country code for {place_name}\") # Keep user-facing warning\n",
    "        logging.warning(f\"Failed to find country code for '{place_name}' after all attempts.\") # Log internal warning\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during geocoding: {e}\") # Keep user-facing error\n",
    "        logging.error(f\"Exception during geocoding process for '{place_name}': {e}\", exc_info=True) # Log internal error\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002b19b-dc3c-488b-95ee-157a02dad7d8",
   "metadata": {},
   "source": [
    "# Section 4: Download and Process OSM Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bec8fca7-d1d6-4ea3-bdae-3b417dcfa564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_osm_graph(polygon):\n",
    "    \"\"\"\n",
    "    Downloads the OSM graph for the specified polygon, projects it,\n",
    "    and returns projected nodes, edges, and the projected graph object.\n",
    "    \"\"\"\n",
    "    # Download more stuff than the standard ones from OSMnx\n",
    "    ox.settings.useful_tags_way = {\n",
    "        \"lanes\": True,\n",
    "        \"highway\": True,\n",
    "        \"maxspeed\": True,\n",
    "        \"cycleway\": [\"lane\", \"opposite_lane\", \"shared_lane\", \"share_busway\", \"opposite_share_busway\"],\n",
    "        \"cycleway:both\": [\"lane\", \"shared_lane\", \"share_busway\"],\n",
    "        \"cycleway:left\": [\"lane\", \"opposite_lane\", \"shared_lane\", \"share_busway\", \"opposite_share_busway\"],\n",
    "        \"cycleway:right\": [\"lane\", \"opposite_lane\", \"shared_lane\", \"share_busway\", \"opposite_share_busway\"],\n",
    "        \"busway\": True,\n",
    "        \"busway:both\": True,\n",
    "        \"busway:right\": True,\n",
    "        \"busway:left\": True,\n",
    "        \"lanes:bus\": True,\n",
    "        \"lanes:psv\": True\n",
    "    }\n",
    "\n",
    "    logging.info(\"Downloading OSM graph from polygon...\")\n",
    "    try:\n",
    "        # Download graph using polygon\n",
    "        G = ox.graph_from_polygon(polygon, network_type=\"drive\", truncate_by_edge=True)\n",
    "        logging.info(\"OSM graph download successful.\")\n",
    "\n",
    "        # --- Project the graph ---\n",
    "        logging.debug(f\"Projecting graph to CRS: {UTM_CRS}\")\n",
    "        G_projected = ox.project_graph(G, to_crs=UTM_CRS)\n",
    "        logging.debug(\"Graph projection successful.\")\n",
    "        # ------------------------\n",
    "\n",
    "    except Exception as e_graph_poly:\n",
    "         logging.error(f\"Failed to download or project graph from polygon: {e_graph_poly}\", exc_info=True)\n",
    "         # Return empty GDFs and None for the graph on failure\n",
    "         return gpd.GeoDataFrame(), gpd.GeoDataFrame(), None # Add None for graph\n",
    "\n",
    "    # Make undirected (optional, depends on analysis needs)\n",
    "    G_projected = ox.utils_graph.get_undirected(G_projected)\n",
    "\n",
    "    try:\n",
    "        # --- Convert PROJECTED graph to GDFs ---\n",
    "        logging.debug(\"Converting projected graph to GeoDataFrames...\")\n",
    "        # Pass projected graph to graph_to_gdfs\n",
    "        gdf_nodes_proj, gdf_edges_proj = ox.graph_to_gdfs(G_projected)\n",
    "        logging.debug(\"Graph conversion successful.\")\n",
    "        # ----------------------------------------\n",
    "\n",
    "        # Reset index and ensure correct types AFTER conversion\n",
    "        gdf_edges_proj = gdf_edges_proj.reset_index()\n",
    "        if 'u' in gdf_edges_proj.columns: gdf_edges_proj['u'] = gdf_edges_proj['u'].astype(int)\n",
    "        if 'v' in gdf_edges_proj.columns: gdf_edges_proj['v'] = gdf_edges_proj['v'].astype(int)\n",
    "\n",
    "    except Exception as e_gdfs:\n",
    "        logging.error(f\"Failed to convert graph to GeoDataFrames: {e_gdfs}\", exc_info=True)\n",
    "        # Return empty GDFs and the potentially valid projected graph (or None if download failed)\n",
    "        return gpd.GeoDataFrame(), gpd.GeoDataFrame(), G_projected if 'G_projected' in locals() else None\n",
    "\n",
    "    # --- Modify return statement ---\n",
    "    # Return projected nodes, projected edges, and the projected graph object\n",
    "    return gdf_nodes_proj, gdf_edges_proj, G_projected\n",
    "    # --- End modification ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4168441b-7680-43c1-a578-91766525b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_way_tags(gdf_edges, combined_all):\n",
    "    \"\"\"\n",
    "    Processes way tags, adds counts, and handles NaN values correctly.  ONLY for 'crossing'.\n",
    "    \"\"\"\n",
    "    gdf_edges_buffered = gdf_edges.copy()\n",
    "    gdf_edges_buffered.geometry = gdf_edges_buffered.geometry.buffer(BUFFER_DISTANCE)\n",
    "    gdf_edges_buffered = gdf_edges_buffered.to_crs(UTM_CRS)\n",
    "    combined_all = combined_all.to_crs(UTM_CRS)\n",
    "\n",
    "    for item_key in way_tags:\n",
    "        if item_key == 'crossing':  # Only process 'crossing' here\n",
    "            if item_key not in combined_all.columns:\n",
    "                logging.warning(f\"Column '{item_key}' not found in combined_all. Skipping processing for this key.\")\n",
    "                # Ensure count columns are created with 0 if the key doesn't exist at all\n",
    "                for item_value in way_tags[item_key]:\n",
    "                     category_name = f\"{item_key}_{item_value}\" if item_value is not True else item_key\n",
    "                     count_col_name = f\"{category_name}_count\"\n",
    "                     if count_col_name not in gdf_edges.columns:\n",
    "                          gdf_edges[count_col_name] = 0\n",
    "                continue # Skip to the next item_key in way_tags\n",
    "\n",
    "            for item_value in way_tags[item_key]:\n",
    "                logging.info(f\"Processing {item_key}={item_value}\")\n",
    "                category_name = f\"{item_key}_{item_value}\" if item_value is not True else item_key\n",
    "                count_col_name = f\"{category_name}_count\" # Define count column name\n",
    "\n",
    "                # Filter combined_all for the specific category\n",
    "                gdf_category = combined_all[(combined_all[item_key] == item_value)]\n",
    "\n",
    "                if not gdf_category.empty:\n",
    "                    # Perform spatial join (ensure CRS match - already done above)\n",
    "                    intersections = gpd.sjoin(gdf_category, gdf_edges, how=\"inner\", predicate='intersects') # Use original gdf_edges here for u,v keys\n",
    "\n",
    "                    # --- Deduplication Logic ---\n",
    "                    unique_crossings = gpd.GeoDataFrame(columns=intersections.columns) # Initialize correctly\n",
    "                    # Ensure correct dtypes if possible, though concat handles it later\n",
    "                    for col, dtype in intersections.dtypes.items():\n",
    "                         if col == 'geometry':\n",
    "                             unique_crossings[col] = gpd.GeoSeries(dtype=dtype, crs=intersections.crs)\n",
    "                         else:\n",
    "                              unique_crossings[col] = pd.Series(dtype=dtype)\n",
    "\n",
    "                    if not intersections.empty: # Only iterate if intersections exist\n",
    "                        for index, row in intersections.iterrows():\n",
    "                            crossing_geom = row['geometry']\n",
    "                            # Ensure 'u' and 'v' exist in the row before accessing\n",
    "                            if 'u' in row and 'v' in row:\n",
    "                                u, v = row['u'], row['v']\n",
    "                                # Check if unique_crossings is not empty before filtering\n",
    "                                if not unique_crossings.empty and 'u' in unique_crossings.columns and 'v' in unique_crossings.columns:\n",
    "                                    existing_crossing = unique_crossings[\n",
    "                                        (unique_crossings['u'] == u) & (unique_crossings['v'] == v)\n",
    "                                        & (unique_crossings.geometry.distance(crossing_geom) < 1e-6) # Use a small tolerance for float comparison\n",
    "                                    ]\n",
    "                                else:\n",
    "                                    existing_crossing = gpd.GeoDataFrame() # Empty if unique_crossings is empty or lacks columns\n",
    "\n",
    "                                if existing_crossing.empty:\n",
    "                                    # Convert row to DataFrame before concat\n",
    "                                    row_gdf = gpd.GeoDataFrame([row], crs=intersections.crs)\n",
    "                                    unique_crossings = pd.concat([unique_crossings, row_gdf], ignore_index=True)\n",
    "                            else:\n",
    "                                logging.warning(f\"Skipping intersection row due to missing 'u' or 'v': {row.get('osmid', 'N/A')}\")\n",
    "                        intersections = unique_crossings\n",
    "                    # --- End Deduplication Logic ---\n",
    "\n",
    "                    if not intersections.empty and 'u' in intersections.columns and 'v' in intersections.columns:\n",
    "                         counts = intersections.groupby(['u', 'v']).size().reset_index(name=count_col_name)\n",
    "                         # Ensure gdf_edges has 'u', 'v' before merging\n",
    "                         if 'u' in gdf_edges.columns and 'v' in gdf_edges.columns:\n",
    "                            gdf_edges = gdf_edges.merge(counts, on=['u', 'v'], how='left')\n",
    "                            # Fill NaN AFTER merge, then convert to int\n",
    "                            gdf_edges[count_col_name] = gdf_edges[count_col_name].fillna(0).astype(int)\n",
    "                         else:\n",
    "                              logging.warning(f\"Cannot merge counts for {category_name} as 'u' or 'v' missing in gdf_edges.\")\n",
    "                              if count_col_name not in gdf_edges.columns: gdf_edges[count_col_name] = 0\n",
    "\n",
    "                    else: # Handle case where intersections became empty or lacked u/v\n",
    "                        if count_col_name not in gdf_edges.columns: gdf_edges[count_col_name] = 0\n",
    "                        else: gdf_edges[count_col_name] = gdf_edges[count_col_name].fillna(0) # Fill existing column if intersections were empty\n",
    "\n",
    "                else: # gdf_category was empty\n",
    "                    if count_col_name not in gdf_edges.columns: gdf_edges[count_col_name] = 0\n",
    "                    else: gdf_edges[count_col_name] = gdf_edges[count_col_name].fillna(0) # Ensure fillna even if category is empty\n",
    "\n",
    "        else:\n",
    "            # If other item_keys were added to way_tags, handle them or skip\n",
    "            continue # Skip if it is highway (or any other key added later)\n",
    "\n",
    "    # Final check to ensure all expected count columns exist and are int\n",
    "    for item_key in way_tags:\n",
    "         if item_key == 'crossing':\n",
    "             for item_value in way_tags[item_key]:\n",
    "                 category_name = f\"{item_key}_{item_value}\" if item_value is not True else item_key\n",
    "                 count_col_name = f\"{category_name}_count\"\n",
    "                 if count_col_name in gdf_edges.columns:\n",
    "                      gdf_edges[count_col_name] = gdf_edges[count_col_name].fillna(0).astype(int)\n",
    "                 else:\n",
    "                      gdf_edges[count_col_name] = 0 # Create if completely missing\n",
    "\n",
    "    return gdf_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "745c1e74-46ca-46e1-a615-1deec8fa4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_traffic_signs(gdf_edges, place, current_city_combined, all_city_mapillary_signs, city_id, data_dir, G_projected, boundary_polygon):\n",
    "    \"\"\"\n",
    "    Processes traffic signs: loads existing combined file if possible, otherwise generates\n",
    "    by loading/downloading OSM (Processed > Raw > Download using the boundary polygon),\n",
    "    conflating with Mapillary, saving results, calculating counts, and processing highway counts.\n",
    "    Uses the pre-downloaded and projected graph G_projected.\n",
    "    Requires the original boundary_polygon used to create G_projected.\n",
    "    \"\"\"\n",
    "    logging.debug(f\"--- Starting process_traffic_signs for {city_id} ---\")\n",
    "\n",
    "    # --- Initialization and Graph Projection (Passed In) ---\n",
    "    if G_projected is None:\n",
    "        logging.error(\"Projected graph (G_projected) is None. Cannot process traffic signs.\")\n",
    "        # Initialize counts to 0 and exit if graph is None\n",
    "        if 'traffic_sign_count' not in gdf_edges.columns: gdf_edges['traffic_sign_count'] = 0\n",
    "        else: gdf_edges['traffic_sign_count'] = gdf_edges['traffic_sign_count'].fillna(0).astype(int)\n",
    "        for item_value in way_tags.get('highway', []): # Ensure hwy counts init\n",
    "            count_col_highway = f\"highway_{item_value}_count\"; gdf_edges[count_col_highway] = gdf_edges.get(count_col_highway, 0)\n",
    "        return gdf_edges\n",
    "\n",
    "    try: # Ensure edges are projected\n",
    "        if gdf_edges.crs != UTM_CRS: gdf_edges_proj = gdf_edges.to_crs(UTM_CRS)\n",
    "        else: gdf_edges_proj = gdf_edges.copy()\n",
    "    except Exception as edge_proj_err:\n",
    "        logging.error(f\"Failed to project gdf_edges to UTM: {edge_proj_err}. Cannot proceed.\", exc_info=True)\n",
    "        return gdf_edges # Cannot proceed without projected edges\n",
    "\n",
    "    base_crs = gdf_edges.crs # Original CRS of edges (or UTM if already projected)\n",
    "\n",
    "    # --- Define Filepaths ---\n",
    "    osm_traffic_signs_raw_filepath = os.path.join(data_dir, city_id, \"raw\", f\"OSM_raw_{city_id}_traffic_signs.geojson\")\n",
    "    osm_traffic_signs_processed_filepath = os.path.join(data_dir, city_id, \"OSM\", f\"OSM_{city_id}_traffic_signs.geojson\")\n",
    "    combined_traffic_signs_filepath = os.path.join(data_dir, city_id, \"combined\", f\"combined_{city_id}_traffic_signs.geojson\")\n",
    "    missing_traffic_signs_filepath = os.path.join(data_dir, city_id, \"missing\", f\"missing_{city_id}_traffic_signs.geojson\")\n",
    "    missing_no_country_tag_filepath = os.path.join(data_dir, city_id, \"missing\", f\"missing_{city_id}_traffic_signs_no_country_tag.geojson\")\n",
    "\n",
    "    # --- Check if Combined Traffic Signs File Exists (Priority 1) ---\n",
    "    signs_loaded = False\n",
    "    combined_signs_for_counting = gpd.GeoDataFrame(crs=UTM_CRS) # Initialize empty GDF with target CRS\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Checking for existing combined traffic signs file: {combined_traffic_signs_filepath}\")\n",
    "        combined_signs_for_counting = gpd.read_file(combined_traffic_signs_filepath)\n",
    "        logging.info(f\"LOADED {len(combined_signs_for_counting)} features from existing combined traffic signs file.\")\n",
    "\n",
    "        # Ensure loaded data is in UTM_CRS for counting\n",
    "        if combined_signs_for_counting.crs != UTM_CRS:\n",
    "            logging.debug(f\"Projecting loaded combined signs from {combined_signs_for_counting.crs} to {UTM_CRS}\")\n",
    "            combined_signs_for_counting = combined_signs_for_counting.to_crs(UTM_CRS)\n",
    "\n",
    "        signs_loaded = True\n",
    "        if not os.path.exists(missing_traffic_signs_filepath):\n",
    "             logging.warning(f\"Combined traffic signs loaded, but missing file not found: {missing_traffic_signs_filepath}\")\n",
    "        if not os.path.exists(missing_no_country_tag_filepath):\n",
    "             logging.warning(f\"Combined traffic signs loaded, but missing (no country tag) file not found: {missing_no_country_tag_filepath}\")\n",
    "\n",
    "    except (FileNotFoundError, DriverError, Exception) as load_err:\n",
    "        logging.info(f\"Combined traffic signs file not found or failed to load ({type(load_err).__name__}). Will generate files.\")\n",
    "        signs_loaded = False\n",
    "        combined_signs_for_counting = gpd.GeoDataFrame(crs=UTM_CRS) # Ensure it's reset\n",
    "\n",
    "    # --- Generate Sign Files ONLY if Not Loaded ---\n",
    "    if not signs_loaded:\n",
    "        gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs) # Initialize for generation path\n",
    "        osm_data_source = \"None\" # Track source\n",
    "\n",
    "        # --- 1. Acquire and Process OSM Traffic Signs (Processed > Raw > Download) ---\n",
    "        try:\n",
    "            # --- Attempt 1: Load Processed OSM ---\n",
    "            logging.info(f\"Attempting to load processed OSM traffic signs: {osm_traffic_signs_processed_filepath}\")\n",
    "            gdf_traffic_signs_processed = gpd.read_file(osm_traffic_signs_processed_filepath)\n",
    "            # Ensure correct base_crs if loaded\n",
    "            if gdf_traffic_signs_processed.crs != base_crs:\n",
    "                 gdf_traffic_signs_processed = gdf_traffic_signs_processed.to_crs(base_crs)\n",
    "            osm_data_source = \"Processed\"\n",
    "            logging.info(f\"LOADED {len(gdf_traffic_signs_processed)} features from processed OSM file.\")\n",
    "\n",
    "        except (FileNotFoundError, DriverError, Exception) as e1:\n",
    "            logging.info(f\"Processed OSM TS file fail ({type(e1).__name__}). Trying raw file.\")\n",
    "            try:\n",
    "                # --- Attempt 2: Load Raw OSM ---\n",
    "                logging.info(f\"Attempting to load raw OSM traffic signs: {osm_traffic_signs_raw_filepath}\")\n",
    "                gdf_traffic_signs_raw = gpd.read_file(osm_traffic_signs_raw_filepath)\n",
    "                osm_data_source = \"Raw\"\n",
    "                logging.info(f\"LOADED {len(gdf_traffic_signs_raw)} features from raw OSM file.\")\n",
    "\n",
    "                # --- Process Raw Data ---\n",
    "                if not gdf_traffic_signs_raw.empty:\n",
    "                    gdf_traffic_signs_processed = gdf_traffic_signs_raw.copy()\n",
    "                    try: # Project to base_crs\n",
    "                         if gdf_traffic_signs_processed.crs != base_crs:\n",
    "                             gdf_traffic_signs_processed = gdf_traffic_signs_processed.to_crs(base_crs)\n",
    "                    except Exception as crs_err:\n",
    "                         logging.error(f\"Failed proj raw OSM TS: {crs_err}\")\n",
    "                         gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs) # Reset\n",
    "\n",
    "                    if 'traffic_sign' in gdf_traffic_signs_processed.columns:\n",
    "                         gdf_traffic_signs_processed = gdf_traffic_signs_processed[gdf_traffic_signs_processed['traffic_sign'].notna()]\n",
    "                    else: # Tag missing\n",
    "                         logging.warning(\"Column 'traffic_sign' not found in loaded raw OSM geometries.\")\n",
    "                         gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs) # Reset\n",
    "\n",
    "                    logging.info(f\"Processed raw OSM data: {len(gdf_traffic_signs_processed)} features remain.\")\n",
    "\n",
    "                    # --- Save Processed (since generated from raw) ---\n",
    "                    if not gdf_traffic_signs_processed.empty:\n",
    "                         gdf_ts_proc_to_save = convert_lists_to_strings(gdf_traffic_signs_processed.copy())\n",
    "                         cols_to_drop_osm_proc = ['element_type', 'osmid', 'nodes']\n",
    "                         gdf_ts_proc_to_save = gdf_ts_proc_to_save.drop(columns=cols_to_drop_osm_proc, errors='ignore')\n",
    "                         if not gdf_ts_proc_to_save.geometry.is_valid.all(): # Validation\n",
    "                              try:\n",
    "                                  orig_crs = gdf_ts_proc_to_save.crs\n",
    "                                  gdf_ts_proc_to_save.geometry = gdf_ts_proc_to_save.geometry.buffer(0)\n",
    "                                  gdf_ts_proc_to_save = gdf_ts_proc_to_save[~gdf_ts_proc_to_save.geometry.is_empty]\n",
    "                                  if not gdf_ts_proc_to_save.empty and gdf_ts_proc_to_save.crs != orig_crs:\n",
    "                                       gdf_ts_proc_to_save = gdf_ts_proc_to_save.set_crs(orig_crs, allow_override=True)\n",
    "                              except Exception as buf_err: logging.error(f\"Buffer error proc OSM (raw): {buf_err}\")\n",
    "\n",
    "                         if not gdf_ts_proc_to_save.empty:\n",
    "                              try:\n",
    "                                  gdf_ts_proc_to_save.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\")\n",
    "                                  logging.info(f\"Saved processed OSM TS file (from raw): {osm_traffic_signs_processed_filepath}\")\n",
    "                              except Exception as save_proc_err:\n",
    "                                  logging.error(f\"Failed save processed OSM TS (raw): {save_proc_err}\")\n",
    "                         else: # Buffer empty\n",
    "                              empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=base_crs)\n",
    "                              schema = create_schema(empty_gdf)\n",
    "                              try:\n",
    "                                  empty_gdf.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                              except Exception as e:\n",
    "                                  logging.error(f\"Failed save empty proc OSM TS (raw, buffer): {e}\")\n",
    "                    else: # Processing raw empty\n",
    "                         empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=base_crs)\n",
    "                         schema = create_schema(empty_gdf)\n",
    "                         try:\n",
    "                             empty_gdf.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                         except Exception as e:\n",
    "                             logging.error(f\"Failed save empty proc OSM TS (raw): {e}\")\n",
    "                else: # Raw file was empty\n",
    "                     gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs)\n",
    "                     empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=base_crs)\n",
    "                     schema = create_schema(empty_gdf)\n",
    "                     try:\n",
    "                         empty_gdf.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                     except Exception as e:\n",
    "                         logging.error(f\"Failed save empty proc OSM TS (empty raw): {e}\")\n",
    "\n",
    "            except (FileNotFoundError, DriverError, Exception) as e2:\n",
    "                logging.info(f\"Raw OSM TS file fail ({type(e2).__name__}). Attempting download.\")\n",
    "                try:\n",
    "                    # --- Attempt 3: Download OSM ---\n",
    "                    # --- Check if boundary_polygon is valid ---\n",
    "                    if boundary_polygon is None or not hasattr(boundary_polygon, 'geom_type') or not boundary_polygon.is_valid:\n",
    "                        logging.error(\"Valid boundary_polygon is required for download but was not provided or is invalid.\")\n",
    "                        raise ValueError(\"Invalid boundary polygon for download.\")\n",
    "                    # -------------------------------------------\n",
    "\n",
    "                    logging.info(f\"Downloading OSM traffic signs using provided boundary polygon...\")\n",
    "                    gdf_traffic_signs_raw = ox.geometries_from_polygon(boundary_polygon, tags=traffic_sign_tags) # <-- USE boundary_polygon\n",
    "                    logging.info(f\"DOWNLOADED {len(gdf_traffic_signs_raw)} raw OSM traffic sign features.\")\n",
    "                    osm_data_source = \"Download\"\n",
    "\n",
    "                    # --- Save Raw (since downloaded) ---\n",
    "                    if not gdf_traffic_signs_raw.empty:\n",
    "                        gdf_ts_raw_to_save = convert_lists_to_strings(gdf_traffic_signs_raw.copy())\n",
    "                        try:\n",
    "                            gdf_ts_raw_to_save.to_file(osm_traffic_signs_raw_filepath, driver=\"GeoJSON\") # No schema\n",
    "                        except Exception as save_raw_err:\n",
    "                            logging.error(f\"Failed save downloaded raw OSM TS: {save_raw_err}\")\n",
    "                    else: # Save empty raw if download empty\n",
    "                         empty_gdf_ts_raw_d = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs='EPSG:4326'); schema_ts_raw_d = create_schema(empty_gdf_ts_raw_d)\n",
    "                         try:\n",
    "                             empty_gdf_ts_raw_d.to_file(osm_traffic_signs_raw_filepath, driver=\"GeoJSON\", schema=schema_ts_raw_d)\n",
    "                         except Exception as e:\n",
    "                             logging.error(f\"Failed save empty raw OSM TS (download empty): {e}\")\n",
    "\n",
    "                    # --- Process Downloaded Data ---\n",
    "                    if not gdf_traffic_signs_raw.empty:\n",
    "                        gdf_traffic_signs_processed = gdf_traffic_signs_raw.copy()\n",
    "                        try: # Project\n",
    "                             if gdf_traffic_signs_processed.crs != base_crs:\n",
    "                                 gdf_traffic_signs_processed = gdf_traffic_signs_processed.to_crs(base_crs)\n",
    "                        except Exception as crs_err_d:\n",
    "                             logging.error(f\"Failed proj downloaded OSM TS: {crs_err_d}\")\n",
    "                             gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs) # Reset\n",
    "                        # Filter\n",
    "                        if 'traffic_sign' in gdf_traffic_signs_processed.columns:\n",
    "                             gdf_traffic_signs_processed = gdf_traffic_signs_processed[gdf_traffic_signs_processed['traffic_sign'].notna()]\n",
    "                        else:\n",
    "                             logging.warning(\"Column 'traffic_sign' not found in downloaded OSM geometries.\")\n",
    "                             gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs) # Reset\n",
    "                        logging.info(f\"Processed downloaded OSM data: {len(gdf_traffic_signs_processed)} features remain.\")\n",
    "                    else: # Download was empty\n",
    "                         gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs)\n",
    "\n",
    "                    # --- Save Processed (since generated from download) ---\n",
    "                    if not gdf_traffic_signs_processed.empty:\n",
    "                         gdf_ts_proc_to_save = convert_lists_to_strings(gdf_traffic_signs_processed.copy())\n",
    "                         cols_to_drop_osm_proc = ['element_type', 'osmid', 'nodes']\n",
    "                         gdf_ts_proc_to_save = gdf_ts_proc_to_save.drop(columns=cols_to_drop_osm_proc, errors='ignore')\n",
    "                         # Validation buffer(0) logic\n",
    "                         if not gdf_ts_proc_to_save.geometry.is_valid.all():\n",
    "                              try:\n",
    "                                   orig_crs = gdf_ts_proc_to_save.crs\n",
    "                                   gdf_ts_proc_to_save.geometry = gdf_ts_proc_to_save.geometry.buffer(0)\n",
    "                                   gdf_ts_proc_to_save = gdf_ts_proc_to_save[~gdf_ts_proc_to_save.geometry.is_empty]\n",
    "                                   if not gdf_ts_proc_to_save.empty and gdf_ts_proc_to_save.crs != orig_crs:\n",
    "                                        gdf_ts_proc_to_save = gdf_ts_proc_to_save.set_crs(orig_crs, allow_override=True)\n",
    "                              except Exception as buf_err: logging.error(f\"Buffer error proc OSM (download): {buf_err}\")\n",
    "\n",
    "                         if not gdf_ts_proc_to_save.empty:\n",
    "                              try:\n",
    "                                  gdf_ts_proc_to_save.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\")\n",
    "                                  logging.info(f\"Saved processed OSM TS file (from download): {osm_traffic_signs_processed_filepath}\")\n",
    "                              except Exception as save_proc_err_d:\n",
    "                                  logging.error(f\"Failed save processed OSM TS (download): {save_proc_err_d}\")\n",
    "                         else: # Buffer empty\n",
    "                              empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=base_crs)\n",
    "                              schema = create_schema(empty_gdf)\n",
    "                              try:\n",
    "                                  empty_gdf.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                              except Exception as e:\n",
    "                                  logging.error(f\"Failed save empty proc OSM TS (download, buffer): {e}\")\n",
    "                    else: # Process download empty\n",
    "                         empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=base_crs)\n",
    "                         schema = create_schema(empty_gdf)\n",
    "                         try:\n",
    "                             empty_gdf.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                         except Exception as e:\n",
    "                             logging.error(f\"Failed save empty proc OSM TS (download): {e}\")\n",
    "\n",
    "                except (ox._errors.InsufficientResponseError, ValueError, Exception) as e3: # Catch ValueError for polygon issue\n",
    "                    logging.error(f\"OSM traffic sign download failed: {e3}\", exc_info=True)\n",
    "                    gdf_traffic_signs_processed = gpd.GeoDataFrame(geometry=[], crs=base_crs) # Ensure empty\n",
    "                    # Save empty files if they don't exist\n",
    "                    if not os.path.exists(osm_traffic_signs_raw_filepath):\n",
    "                         empty_gdf_ts_raw_err = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs='EPSG:4326'); schema_ts_raw_err = create_schema(empty_gdf_ts_raw_err)\n",
    "                         try: empty_gdf_ts_raw_err.to_file(osm_traffic_signs_raw_filepath, driver=\"GeoJSON\", schema=schema_ts_raw_err)\n",
    "                         except Exception as e_save: logging.error(f\"Failed save empty raw OSM TS on download error: {e_save}\")\n",
    "                    if not os.path.exists(osm_traffic_signs_processed_filepath):\n",
    "                         empty_gdf_ts_proc_err = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=base_crs); schema_ts_proc_err = create_schema(empty_gdf_ts_proc_err)\n",
    "                         try: empty_gdf_ts_proc_err.to_file(osm_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema_ts_proc_err)\n",
    "                         except Exception as e_save: logging.error(f\"Failed save empty processed OSM TS on download error: {e_save}\")\n",
    "\n",
    "        # --- End OSM Data Acquisition ---\n",
    "\n",
    "        # --- 1.5 Conflate OSM and Mapillary Traffic Signs ---\n",
    "        logging.info(f\"--- Starting Traffic Sign Conflation (using OSM from {osm_data_source}) ---\")\n",
    "        combined_signs = gpd.GeoDataFrame(crs=UTM_CRS); missing_signs = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "        osm_signs_proj = gpd.GeoDataFrame(crs=UTM_CRS); mply_signs_proj = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "\n",
    "        # --- Prepare OSM signs ---\n",
    "        if isinstance(gdf_traffic_signs_processed, gpd.GeoDataFrame) and not gdf_traffic_signs_processed.empty:\n",
    "             try:\n",
    "                  if gdf_traffic_signs_processed.crs != UTM_CRS: osm_signs_proj = gdf_traffic_signs_processed.to_crs(UTM_CRS)\n",
    "                  else: osm_signs_proj = gdf_traffic_signs_processed.copy()\n",
    "                  osm_signs_proj['source'] = 'OSM'\n",
    "                  if 'traffic_sign' not in osm_signs_proj.columns: osm_signs_proj['traffic_sign'] = ''\n",
    "                  else: osm_signs_proj['traffic_sign'] = osm_signs_proj['traffic_sign'].astype(str).str.strip().replace(['nan', 'None', 'NULL'], '', regex=False)\n",
    "                  logging.debug(f\"CONFLATION - Prepared {len(osm_signs_proj)} OSM signs.\")\n",
    "             except Exception as osm_proj_err: logging.error(f\"CONFLATION - Failed prep OSM signs: {osm_proj_err}\")\n",
    "\n",
    "        # --- Prepare Mapillary signs ---\n",
    "        if isinstance(all_city_mapillary_signs, gpd.GeoDataFrame) and not all_city_mapillary_signs.empty:\n",
    "             try:\n",
    "                  if all_city_mapillary_signs.crs != UTM_CRS: mply_signs_proj = all_city_mapillary_signs.to_crs(UTM_CRS)\n",
    "                  else: mply_signs_proj = all_city_mapillary_signs.copy()\n",
    "                  if 'source' not in mply_signs_proj.columns: mply_signs_proj['source'] = 'Mapillary'\n",
    "\n",
    "                  # --- Tag Extraction Logic ---\n",
    "                  logging.debug(\"CONFLATION - Determining 'traffic_sign' tag for Mapillary...\")\n",
    "                  if 'traffic_sign' not in mply_signs_proj.columns: mply_signs_proj['traffic_sign'] = ''\n",
    "                  mply_signs_proj['traffic_sign'] = mply_signs_proj['traffic_sign'].astype(str).str.strip().replace(['nan', 'None', 'NULL'], '', regex=False)\n",
    "                  needs_derivation = mply_signs_proj['traffic_sign'] == ''\n",
    "\n",
    "                  if needs_derivation.any() and 'country_tag' in mply_signs_proj.columns:\n",
    "                       logging.debug(\"CONFLATION - Trying 'country_tag'...\")\n",
    "                       split_tags = mply_signs_proj.loc[needs_derivation, 'country_tag'].astype(str).str.split('traffic_sign=', n=1, expand=True)\n",
    "                       derived_country = split_tags[1].where(split_tags[1].notna(), split_tags[0]).str.strip().replace(['nan', 'None', 'NULL'], '', regex=False)\n",
    "                       mply_signs_proj.loc[needs_derivation, 'traffic_sign'] = mply_signs_proj.loc[needs_derivation, 'traffic_sign'].replace('', np.nan).fillna(derived_country)\n",
    "                       needs_derivation = mply_signs_proj['traffic_sign'] == ''\n",
    "\n",
    "                  if needs_derivation.any() and 'osm_tag_1' in mply_signs_proj.columns:\n",
    "                       logging.debug(\"CONFLATION - Trying 'osm_tag_1'...\")\n",
    "                       looks_like_kv = mply_signs_proj.loc[needs_derivation, 'osm_tag_1'].astype(str).str.contains('=', na=False)\n",
    "                       if looks_like_kv.any():\n",
    "                           split_tags = mply_signs_proj.loc[needs_derivation & looks_like_kv, 'osm_tag_1'].astype(str).str.split('=', n=1, expand=True)\n",
    "                           derived_osm = split_tags[1].where(split_tags[1].notna(), split_tags[0]).str.strip().replace(['nan', 'None', 'NULL'], '', regex=False)\n",
    "                           mply_signs_proj.loc[needs_derivation & looks_like_kv, 'traffic_sign'] = mply_signs_proj.loc[needs_derivation & looks_like_kv, 'traffic_sign'].replace('', np.nan).fillna(derived_osm)\n",
    "                       needs_derivation = mply_signs_proj['traffic_sign'] == ''\n",
    "\n",
    "                  if needs_derivation.any() and 'mapillary_feature' in mply_signs_proj.columns:\n",
    "                       logging.debug(\"CONFLATION - Falling back to 'mapillary_feature'...\")\n",
    "                       derived_raw = mply_signs_proj.loc[needs_derivation, 'mapillary_feature'].astype(str).str.strip().replace(['nan', 'None', 'NULL'], '', regex=False)\n",
    "                       mply_signs_proj.loc[needs_derivation, 'traffic_sign'] = mply_signs_proj.loc[needs_derivation, 'traffic_sign'].replace('', np.nan).fillna(derived_raw)\n",
    "\n",
    "                  mply_signs_proj['traffic_sign'] = mply_signs_proj['traffic_sign'].fillna('').astype(str)\n",
    "                  # --- End Tag Extraction Logic ---\n",
    "                  logging.debug(f\"CONFLATION - Prepared {len(mply_signs_proj)} Mapillary signs.\")\n",
    "                  if not mply_signs_proj.empty: logging.debug(f\"CONFLATION - Unique Mapillary 'traffic_sign' tags after derivation: {mply_signs_proj['traffic_sign'].unique()}\")\n",
    "\n",
    "             except Exception as mply_prep_err: logging.error(f\"CONFLATION - Failed prep Mapillary: {mply_prep_err}\"); mply_signs_proj = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "\n",
    "        # --- Perform conflation ---\n",
    "        if not osm_signs_proj.empty and not mply_signs_proj.empty:\n",
    "             # (Buffer, sjoin, loop/compare tags, build matched_mply_indices)\n",
    "             osm_buffered = osm_signs_proj[osm_signs_proj.geometry.is_valid].copy()\n",
    "             mply_buffered = mply_signs_proj[mply_signs_proj.geometry.is_valid].copy()\n",
    "             if osm_buffered.empty or mply_buffered.empty:\n",
    "                 combined_signs = pd.concat([osm_buffered, mply_buffered], ignore_index=True, sort=False)\n",
    "                 combined_signs = gpd.GeoDataFrame(combined_signs, geometry='geometry', crs=UTM_CRS)\n",
    "                 missing_signs = mply_buffered.copy() if not osm_buffered.empty else gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "             else:\n",
    "                 osm_buffered['geometry'] = osm_buffered.geometry.buffer(BUFFER_DISTANCE); mply_buffered['geometry'] = mply_buffered.geometry.buffer(BUFFER_DISTANCE)\n",
    "                 possible_matches = gpd.sjoin(mply_buffered, osm_buffered, how='inner', predicate='intersects', lsuffix='mply', rsuffix='osm')\n",
    "                 matched_mply_indices = set(); osm_index_col = 'index_osm' if 'index_osm' in possible_matches.columns else 'index_right'\n",
    "                 missing_mply_indices = mply_signs_proj.index\n",
    "                 if osm_index_col not in possible_matches.columns and not possible_matches.empty: pass\n",
    "                 elif not possible_matches.empty:\n",
    "                     for mply_idx, group in possible_matches.groupby(possible_matches.index):\n",
    "                          if mply_idx in matched_mply_indices or mply_idx not in mply_signs_proj.index: continue\n",
    "                          mply_sign_tag = mply_signs_proj.loc[mply_idx, 'traffic_sign']\n",
    "                          if not mply_sign_tag: continue\n",
    "                          for osm_match_idx in group[osm_index_col].unique():\n",
    "                               if osm_match_idx not in osm_signs_proj.index: continue\n",
    "                               osm_sign_tag = osm_signs_proj.loc[osm_match_idx, 'traffic_sign']\n",
    "                               if not osm_sign_tag:\n",
    "                                   continue\n",
    "                               if mply_sign_tag.lower() == osm_sign_tag.lower():\n",
    "                                   matched_mply_indices.add(mply_idx); break\n",
    "                     missing_mply_indices = mply_signs_proj.index.difference(matched_mply_indices)\n",
    "                 missing_signs = mply_signs_proj.loc[missing_mply_indices].copy()\n",
    "                 try:\n",
    "                     combined_signs = pd.concat([osm_signs_proj, missing_signs], ignore_index=True, sort=False)\n",
    "                     combined_signs = gpd.GeoDataFrame(combined_signs, geometry='geometry', crs=UTM_CRS)\n",
    "                 except Exception as concat_err:\n",
    "                     combined_signs = osm_signs_proj.copy() # Fallback\n",
    "        elif not osm_signs_proj.empty:\n",
    "            combined_signs = osm_signs_proj.copy()\n",
    "            missing_signs = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "        elif not mply_signs_proj.empty:\n",
    "            combined_signs = mply_signs_proj.copy()\n",
    "            missing_signs = mply_signs_proj.copy()\n",
    "        else:\n",
    "            combined_signs = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "            missing_signs = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "\n",
    "\n",
    "        # --- Save Conflation Results (Apply Tag Processing Before Save) ---\n",
    "        cols_to_drop_save = ['index_mply', 'index_osm', 'index_right', 'index_left', 'geom_wkt_temp', 'element_type', 'osmid', 'nodes'] + [f'osm_tag_{i}_country' for i in range(1,6)] + [f'traffic_sign_{suffix}' for suffix in ['mply','osm','left','right']]\n",
    "\n",
    "        # Save Combined Signs\n",
    "        logging.info(f\"Saving {len(combined_signs)} combined traffic signs to: {combined_traffic_signs_filepath}\")\n",
    "        if not combined_signs.empty:\n",
    "            combined_signs_processed_tags, _ = convert_osm_tags_to_kv(combined_signs.copy())\n",
    "            if isinstance(combined_signs_processed_tags, gpd.GeoDataFrame) and not combined_signs_processed_tags.empty:\n",
    "                combined_signs_to_save = convert_lists_to_strings(combined_signs_processed_tags).drop(columns=cols_to_drop_save, errors='ignore')\n",
    "                try: combined_signs_to_save.to_file(combined_traffic_signs_filepath, driver=\"GeoJSON\")\n",
    "                except Exception as save_comb_ts_err: logging.error(f\"!!! Failed save combined TS: {save_comb_ts_err}\")\n",
    "            else: # Tag fail\n",
    "                 empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                 schema = create_schema(empty_gdf)\n",
    "                 try:\n",
    "                     empty_gdf.to_file(combined_traffic_signs_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                 except Exception as e:\n",
    "                     pass\n",
    "        else: # Empty combined\n",
    "             empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "             schema = create_schema(empty_gdf)\n",
    "             try:\n",
    "                 empty_gdf.to_file(combined_traffic_signs_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "             except Exception as e:\n",
    "                 pass\n",
    "\n",
    "        # --- FILTER and Save Missing Signs ---\n",
    "        missing_signs_filtered = gpd.GeoDataFrame(crs=UTM_CRS); missing_signs_no_country_tag = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "        if not missing_signs.empty:\n",
    "            if 'country_tag' in missing_signs.columns:\n",
    "                condition_not_na = missing_signs['country_tag'].notna()\n",
    "                condition_not_empty_or_null_str = missing_signs['country_tag'].astype(str).str.strip().isin(['', 'nan', 'None', 'NULL']) == False\n",
    "                rows_to_keep = condition_not_na & condition_not_empty_or_null_str; rows_to_discard = ~rows_to_keep\n",
    "                missing_signs_filtered = missing_signs[rows_to_keep].copy(); missing_signs_no_country_tag = missing_signs[rows_to_discard].copy()\n",
    "            else: missing_signs_no_country_tag = missing_signs.copy(); missing_signs_filtered = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "        else: pass # Both remain empty\n",
    "\n",
    "        # Save Filtered Missing Signs (WITH country tag)\n",
    "        logging.info(f\"Saving {len(missing_signs_filtered)} filtered missing traffic signs to: {missing_traffic_signs_filepath}\")\n",
    "        if not missing_signs_filtered.empty:\n",
    "            missing_signs_processed_tags, _ = convert_osm_tags_to_kv(missing_signs_filtered.copy())\n",
    "            if isinstance(missing_signs_processed_tags, gpd.GeoDataFrame) and not missing_signs_processed_tags.empty:\n",
    "                missing_signs_to_save = convert_lists_to_strings(missing_signs_processed_tags).drop(columns=cols_to_drop_save, errors='ignore')\n",
    "                try:\n",
    "                    missing_signs_to_save.to_file(missing_traffic_signs_filepath, driver=\"GeoJSON\")\n",
    "                except Exception as save_miss_ts_err:\n",
    "                    logging.error(f\"!!! Failed save filtered missing TS: {save_miss_ts_err}\")\n",
    "            else: # Tag fail\n",
    "                 empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                 schema = create_schema(empty_gdf)\n",
    "                 try:\n",
    "                     empty_gdf.to_file(missing_traffic_signs_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                 except Exception as e:\n",
    "                     pass\n",
    "        else: # Empty filtered\n",
    "             empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "             schema = create_schema(empty_gdf)\n",
    "             try:\n",
    "                 empty_gdf.to_file(missing_traffic_signs_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "             except Exception as e:\n",
    "                 pass\n",
    "\n",
    "        # Save Missing Signs WITHOUT country tag\n",
    "        logging.info(f\"Saving {len(missing_signs_no_country_tag)} missing signs without country tag to: {missing_no_country_tag_filepath}\")\n",
    "        if not missing_signs_no_country_tag.empty:\n",
    "            missing_no_ct_processed_tags, _ = convert_osm_tags_to_kv(missing_signs_no_country_tag.copy())\n",
    "            if isinstance(missing_no_ct_processed_tags, gpd.GeoDataFrame) and not missing_no_ct_processed_tags.empty:\n",
    "                missing_no_ct_to_save = convert_lists_to_strings(missing_no_ct_processed_tags).drop(columns=cols_to_drop_save, errors='ignore')\n",
    "                try: missing_no_ct_to_save.to_file(missing_no_country_tag_filepath, driver=\"GeoJSON\")\n",
    "                except Exception as save_miss_no_ct_err: logging.error(f\"!!! Failed save missing TS (no country tag): {save_miss_no_ct_err}\")\n",
    "            else: # Tag fail\n",
    "                empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                schema = create_schema(empty_gdf)\n",
    "                try:\n",
    "                    empty_gdf.to_file(missing_no_country_tag_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        else: # Empty no country tag\n",
    "            empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "            schema = create_schema(empty_gdf)\n",
    "            try:\n",
    "                empty_gdf.to_file(missing_no_country_tag_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        # --- End Conflation Save Section ---\n",
    "\n",
    "        # Assign the FULL combined signs (tag processed) for counting\n",
    "        if 'combined_signs_processed_tags' in locals() and isinstance(locals().get('combined_signs_processed_tags'), gpd.GeoDataFrame) and not locals().get('combined_signs_processed_tags').empty:\n",
    "             combined_signs_for_counting = locals().get('combined_signs_processed_tags').copy()\n",
    "             logging.debug(\"Using newly generated & tag-processed combined signs for counting.\")\n",
    "        elif isinstance(combined_signs, gpd.GeoDataFrame) and not combined_signs.empty:\n",
    "             combined_signs_for_counting = combined_signs.copy() # Fallback if tag processing failed\n",
    "             logging.warning(\"Using newly generated combined signs *before* tag processing for counting.\")\n",
    "        else:\n",
    "             combined_signs_for_counting = gpd.GeoDataFrame(crs=UTM_CRS) # Ensure empty if generation failed\n",
    "\n",
    "    # --- End File Generation Block ('if not signs_loaded:') ---\n",
    "\n",
    "\n",
    "    # --- 2. Calculate Counts from Combined Traffic Signs ---\n",
    "    logging.info(\"Calculating final traffic sign counts from combined data...\")\n",
    "    # Initialize count column safely\n",
    "    if 'traffic_sign_count' not in gdf_edges.columns: gdf_edges['traffic_sign_count'] = 0\n",
    "    else: gdf_edges['traffic_sign_count'] = gdf_edges['traffic_sign_count'].fillna(0)\n",
    "\n",
    "    if not combined_signs_for_counting.empty and isinstance(combined_signs_for_counting, gpd.GeoDataFrame) and 'geometry' in combined_signs_for_counting.columns:\n",
    "        combined_signs_points = combined_signs_for_counting[combined_signs_for_counting.geometry.geom_type == 'Point'].copy()\n",
    "        if not combined_signs_points.empty:\n",
    "             if combined_signs_points.crs != UTM_CRS: # Project points if needed\n",
    "                 try:\n",
    "                     combined_signs_points = combined_signs_points.to_crs(UTM_CRS)\n",
    "                 except Exception as crs_err_pts:\n",
    "                     combined_signs_points = gpd.GeoDataFrame()\n",
    "\n",
    "             if not combined_signs_points.empty: # Check again after projection\n",
    "                 centroids_comb = combined_signs_points.geometry\n",
    "                 gdf_nearest_edges_comb = ox.distance.nearest_edges(G_projected, X=centroids_comb.x, Y=centroids_comb.y, return_dist=False)\n",
    "                 final_sign_counts = []\n",
    "                 if 'traffic_sign' not in combined_signs_points.columns: combined_signs_points['traffic_sign'] = ''\n",
    "                 for idx in combined_signs_points.index:\n",
    "                    sign_value = combined_signs_points.loc[idx, 'traffic_sign']\n",
    "                    sign_count = 1 + (sign_value.count(';') if isinstance(sign_value, str) and sign_value else 0)\n",
    "                    final_sign_counts.append(sign_count)\n",
    "                 normalized_edges_comb = [(min(u, v), max(u, v)) for u, v, _ in gdf_nearest_edges_comb]\n",
    "                 if len(final_sign_counts) == len(normalized_edges_comb):\n",
    "                    final_intersections = pd.DataFrame({'u': [u for u,v in normalized_edges_comb], 'v': [v for u,v in normalized_edges_comb], 'final_sign_count': final_sign_counts})\n",
    "                    if not final_intersections.empty:\n",
    "                        final_counts_agg = final_intersections.groupby(['u', 'v'])['final_sign_count'].sum().reset_index(name=\"traffic_sign_count_new\")\n",
    "                        gdf_edges = gdf_edges.merge(final_counts_agg, on=['u', 'v'], how='left')\n",
    "                        gdf_edges['traffic_sign_count'] = gdf_edges['traffic_sign_count_new'].fillna(0) # Overwrite\n",
    "                        gdf_edges = gdf_edges.drop(columns=['traffic_sign_count_new']) # Drop temp\n",
    "                        logging.info(f\"Final combined traffic sign counts merged ({int(gdf_edges['traffic_sign_count'].sum())} total signs).\")\n",
    "                    else: gdf_edges['traffic_sign_count'] = 0; logging.info(\"No combined TS intersections.\") # Overwrite\n",
    "                 else: logging.warning(f\"Length mismatch counts/edges. Counts not updated.\"); gdf_edges['traffic_sign_count'] = gdf_edges['traffic_sign_count'].fillna(0)\n",
    "             else: logging.warning(\"Combined points empty after proj. Counts not updated.\"); gdf_edges['traffic_sign_count'] = gdf_edges['traffic_sign_count'].fillna(0)\n",
    "        else: logging.info(\"No Point geoms in combined signs. Counts not updated.\"); gdf_edges['traffic_sign_count'] = gdf_edges['traffic_sign_count'].fillna(0)\n",
    "    else: logging.warning(\"Combined signs data empty/invalid for counting. Counts not updated.\"); gdf_edges['traffic_sign_count'] = gdf_edges['traffic_sign_count'].fillna(0)\n",
    "    # Final assurance: ensure column exists and is integer\n",
    "    gdf_edges['traffic_sign_count'] = gdf_edges.get('traffic_sign_count', 0).fillna(0).astype(int)\n",
    "    # --- End Revised Count Calculation ---\n",
    "\n",
    "\n",
    "    # --- 3. Process 'highway' Counts (using current_city_combined) ---\n",
    "    logging.info(\"Processing highway feature counts\")\n",
    "    highway_tags_to_process = way_tags.get('highway', [])\n",
    "    processed_hwy_input = gpd.GeoDataFrame() # Initialize\n",
    "    if isinstance(current_city_combined, gpd.GeoDataFrame) and not current_city_combined.empty:\n",
    "        if current_city_combined.crs != UTM_CRS:\n",
    "             try: processed_hwy_input = current_city_combined.to_crs(UTM_CRS)\n",
    "             except Exception as crs_err_hwy: logging.error(f\"Failed proj highway input: {crs_err_hwy}\")\n",
    "        else: processed_hwy_input = current_city_combined.copy()\n",
    "    else: logging.warning(\"current_city_combined invalid/empty for highway counts.\")\n",
    "\n",
    "    if not processed_hwy_input.empty and 'highway' in processed_hwy_input.columns and highway_tags_to_process:\n",
    "        # Use processed_hwy_input which is guaranteed to be in UTM_CRS if not empty\n",
    "        for item_value in highway_tags_to_process:\n",
    "            category_name = f\"highway_{item_value}\"; count_col_name = f\"{category_name}_count\"\n",
    "            if count_col_name not in gdf_edges.columns: gdf_edges[count_col_name] = 0\n",
    "            gdf_category = processed_hwy_input[processed_hwy_input['highway'] == item_value]\n",
    "            if not gdf_category.empty:\n",
    "                 gdf_category_buffered = gdf_category.copy()\n",
    "                 if (gdf_category_buffered.geom_type == 'Point').any(): gdf_category_buffered.geometry = gdf_category_buffered.buffer(0.1) # Small buffer\n",
    "                 intersections = gpd.sjoin(gdf_category_buffered, gdf_edges_proj, how=\"inner\", predicate='intersects') # Use projected edges\n",
    "                 if not intersections.empty:\n",
    "                     counts = intersections.groupby(['u', 'v']).size().reset_index(name=f\"{count_col_name}_new\")\n",
    "                     gdf_edges = gdf_edges.merge(counts, on=['u', 'v'], how='left')\n",
    "                     gdf_edges[count_col_name] = gdf_edges[count_col_name].fillna(0) + gdf_edges[f\"{count_col_name}_new\"].fillna(0)\n",
    "                     gdf_edges = gdf_edges.drop(columns=[f\"{count_col_name}_new\"])\n",
    "                     gdf_edges[count_col_name] = gdf_edges[count_col_name].astype(int)\n",
    "                     logging.debug(f\"Highway counts for {category_name} added.\")\n",
    "                 else: gdf_edges[count_col_name] = gdf_edges[count_col_name].fillna(0).astype(int) # Ensure 0 if no intersections\n",
    "            else: gdf_edges[count_col_name] = gdf_edges[count_col_name].fillna(0).astype(int) # Ensure 0 if no category data\n",
    "    else:\n",
    "         logging.warning(\"Skipping highway counts: Input data invalid/empty, 'highway' missing, or no tags defined.\")\n",
    "         for item_value in highway_tags_to_process: gdf_edges[f\"highway_{item_value}_count\"] = gdf_edges.get(f\"highway_{item_value}_count\", 0)\n",
    "\n",
    "\n",
    "    # --- Final CRS Check ---\n",
    "    if gdf_edges.crs != UTM_CRS:\n",
    "        try: gdf_edges = gdf_edges.to_crs(UTM_CRS)\n",
    "        except Exception as final_crs_err: logging.error(f\"Failed final CRS conversion to UTM: {final_crs_err}\")\n",
    "\n",
    "    logging.info(\"Finished processing traffic signs (conflated) and highway counts.\")\n",
    "    return gdf_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8803aa95-90c9-459b-a535-bbe3cb8d387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_network(gdf_edges, filepath):\n",
    "    \"\"\"Saves the processed network GeoDataFrame to a file after converting list-type columns to strings.\"\"\"\n",
    "    list_columns = [col for col in gdf_edges.columns if any(isinstance(val, list) for val in gdf_edges[col])]\n",
    "    for col in list_columns:\n",
    "        gdf_edges[col] = gdf_edges[col].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else x)\n",
    "    gdf_edges.to_file(filepath, driver=\"GPKG\")\n",
    "    print(f\"Processed network saved to '{filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbc82eb3-38c8-4d0c-8a83-65525426a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cycleways_and_busways(gdf_edges):\n",
    "    \"\"\"\n",
    "    Processes cycleways and busways in the OSM data.\n",
    "    \"\"\"\n",
    "\n",
    "    gdf_edges['cycle_lane'] = 'no'\n",
    "    gdf_edges['shared_cycle'] = 'no'\n",
    "    gdf_edges['bus_lane'] = 'no'\n",
    "\n",
    "    for cycleway_col in ['cycleway', 'cycleway:left', 'cycleway:right', 'cycleway:both']:\n",
    "        if cycleway_col in gdf_edges.columns:\n",
    "            gdf_edges['cycle_lane'] = np.where(gdf_edges[cycleway_col].isin(['lane', 'opposite_lane']), 'yes', gdf_edges['cycle_lane'])\n",
    "            gdf_edges['shared_cycle'] = np.where(gdf_edges[cycleway_col].isin(['shared_lane', 'share_busway', 'opposite_share_busway']), 'yes', gdf_edges['shared_cycle'])\n",
    "\n",
    "    for busway_col in ['busway', 'busway:left', 'busway:right', 'busway:both', 'lanes:bus', 'lanes:psv']:\n",
    "        if busway_col in gdf_edges.columns:\n",
    "            # Force string conversion and stripping of whitespace\n",
    "            gdf_edges[busway_col] = gdf_edges[busway_col].astype(str).str.strip()\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(gdf_edges[busway_col]):\n",
    "                gdf_edges['bus_lane'] = np.where(\n",
    "                    (gdf_edges[busway_col].notna()) & (gdf_edges[busway_col] != '0'), # Compare as strings\n",
    "                    'yes',\n",
    "                    gdf_edges['bus_lane']\n",
    "                )\n",
    "            else:\n",
    "                # Explicitly handle common string values, including 'nan'\n",
    "                gdf_edges['bus_lane'] = np.where(\n",
    "                    gdf_edges[busway_col].isin(['yes', 'designated', 'official', 'permissive', '1', 'lane']), # Expanded list\n",
    "                    'yes',\n",
    "                    gdf_edges['bus_lane']\n",
    "                )\n",
    "\n",
    "\n",
    "    gdf_edges['bus_lane'] = gdf_edges['bus_lane'].fillna('no') # Keep\n",
    "\n",
    "    return gdf_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d6d6e-4aaa-40d6-8674-e23d241125bf",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c617486-f5b1-476e-83ec-3c1971323dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user_inputs):\n",
    "    global transport_agency_data_exists, transport_agency_osm_df # Declare globals\n",
    "\n",
    "    # --- Initialize ---\n",
    "    all_gdf_edges = [] # List to track which cities were successfully processed\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True) # Ensure results dir exists\n",
    "    city_processing_log_file = os.path.join(RESULTS_DIR, \"city_processing_times.csv\")\n",
    "    city_times_list = []\n",
    "\n",
    "    # --- Load Mappings ---\n",
    "    mapillary_osm_mapping, country_mappings = load_mappings()\n",
    "\n",
    "    # --- Load Existing Times Log ---\n",
    "    processed_city_ids = set()\n",
    "    if os.path.exists(city_processing_log_file):\n",
    "        try:\n",
    "            existing_times_df = pd.read_csv(city_processing_log_file, sep=';')\n",
    "            city_times_list = existing_times_df.to_dict('records')\n",
    "            processed_city_ids = set(existing_times_df['city_id'])\n",
    "            logging.info(f\"Loaded existing processing times for {len(processed_city_ids)} cities.\")\n",
    "        except Exception as e_load_times:\n",
    "             logging.warning(f\"Could not load existing city times log: {e_load_times}\")\n",
    "             processed_city_ids = set() # Ensure it's a set even if loading fails\n",
    "    # ----------------------------------------\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "    # data_dir is already defined globally as \"data\"\n",
    "\n",
    "    logging.info(\"Starting processing...\")\n",
    "\n",
    "    if transport_agency_data_exists:\n",
    "        logging.info(\"Preprocessing transport agency data...\")\n",
    "        agency_gdf_processed, _ = convert_osm_tags_to_kv(transport_agency_osm_df)\n",
    "        if isinstance(agency_gdf_processed, gpd.GeoDataFrame):\n",
    "                agency_gdf_processed['source'] = 'transit_agency'\n",
    "                transport_agency_osm_df = agency_gdf_processed # Update the global variable\n",
    "                logging.info(\"Transport agency data preprocessed.\")\n",
    "        else:\n",
    "                logging.warning(\"Agency data preprocessing failed.\")\n",
    "                transport_agency_data_exists = False\n",
    "\n",
    "    os.makedirs(DATA_DIR, exist_ok=True) # Ensure data dir exists\n",
    "\n",
    "    for user_input in user_inputs:\n",
    "        logging.info(f\"\\n=== Processing City: {user_input} ===\")\n",
    "        start_city_time = time.time()\n",
    "        city_id = sanitize_filename(user_input)\n",
    "\n",
    "        current_city_combined_list = []\n",
    "        city_mapillary_signs_processed_list = []\n",
    "        processed_mapillary_filters = set()\n",
    "\n",
    "        if city_id in processed_city_ids:\n",
    "            logging.info(f\"City '{city_id}' skipped (already processed according to log).\")\n",
    "            continue\n",
    "\n",
    "        country_code = get_country_code(user_input, country_mappings)\n",
    "        if country_code is None:\n",
    "            logging.warning(f\"No country code for {city_id}. Skipping.\")\n",
    "            continue\n",
    "        city_dir = os.path.join(DATA_DIR, city_id)\n",
    "        create_city_directories(DATA_DIR, city_id)\n",
    "\n",
    "        processed_network_filepath = os.path.join(city_dir, f\"{city_id}_processed_network.gpkg\")\n",
    "        processed_network_loaded = False\n",
    "        gdf_edges = None\n",
    "\n",
    "        try: # Try loading existing network\n",
    "                if not os.path.exists(processed_network_filepath):\n",
    "                    raise FileNotFoundError(\"Network file not found.\")\n",
    "                gdf_edges = gpd.read_file(processed_network_filepath)\n",
    "                if gdf_edges.empty:\n",
    "                    raise ValueError(\"Loaded network file is empty.\")\n",
    "                new_cols = []; seen_cols = set() # Deduplicate columns\n",
    "                for col in gdf_edges.columns:\n",
    "                    current_col_name = col\n",
    "                    if col in seen_cols:\n",
    "                        i = 1\n",
    "                        new_col_candidate = f\"{col}_{i}\"\n",
    "                        while new_col_candidate in seen_cols or new_col_candidate in gdf_edges.columns or new_col_candidate in new_cols:\n",
    "                            i += 1\n",
    "                            new_col_candidate = f\"{col}_{i}\"\n",
    "                        current_col_name = new_col_candidate\n",
    "                    new_cols.append(current_col_name)\n",
    "                    seen_cols.add(current_col_name)\n",
    "                gdf_edges.columns = new_cols\n",
    "                # Don't add to all_gdf_edges here, just confirm load\n",
    "                processed_network_loaded = True\n",
    "                logging.info(f\"Loaded pre-existing network for {city_id}. Skipping generation.\")\n",
    "        except Exception as e_load:\n",
    "            logging.info(f\"Network load failed for {city_id}: {e_load}. Generating.\")\n",
    "            processed_network_loaded = False\n",
    "\n",
    "        if not processed_network_loaded:\n",
    "                try:\n",
    "                    area_gdf, bbox = get_area_from_input(user_input, city_id)\n",
    "                    if area_gdf is None or bbox is None:\n",
    "                        raise ValueError(\"Area GDF or BBox could not be determined.\")\n",
    "\n",
    "                    # --- Filter Loop ---\n",
    "                    for filter_type, filter_values in filters.items():\n",
    "                        if filter_values:\n",
    "                            for filter_value in filter_values:\n",
    "                                    logging.info(f\"--- Processing Filter: {filter_value} ---\")\n",
    "                                    filter_value_str = filter_value.replace(\"=\", \"_\")\n",
    "                                    combined_filepath = os.path.join(city_dir, \"combined\", f\"combined_{city_id}_{filter_value_str}.geojson\")\n",
    "                                    missing_filepath = os.path.join(city_dir, \"missing\", f\"missing_{city_id}_{filter_value_str}.geojson\")\n",
    "                                    osm_processed_filepath = os.path.join(DATA_DIR, city_id, \"OSM\", f\"OSM_{city_id}_{filter_value_str}.geojson\")\n",
    "                                    osm_raw_filepath = os.path.join(DATA_DIR, city_id, \"raw\", f\"OSM_raw_{city_id}_{filter_value_str}.geojson\")\n",
    "                                    mapillary_processed_filepath = os.path.join(DATA_DIR, city_id, \"Mapillary\", f\"Mapillary_{city_id}_{filter_value_str}.geojson\")\n",
    "                                    mapillary_raw_filepath = os.path.join(DATA_DIR, city_id, \"raw\", f\"Mapillary_raw_{city_id}_{filter_value_str}.geojson\")\n",
    "\n",
    "                                    combined = None; combined_loaded = False\n",
    "                                    try: # Load combined check...\n",
    "                                        combined = gpd.read_file(combined_filepath)\n",
    "                                        new_cols = []; seen_cols = set(); # Dedupe...\n",
    "                                        for col in combined.columns:\n",
    "                                            current_col_name = col\n",
    "                                            if col in seen_cols:\n",
    "                                                i = 1\n",
    "                                            new_col_candidate = f\"{col}_{i}\"\n",
    "                                            while new_col_candidate in seen_cols or new_col_candidate in combined.columns or new_col_candidate in new_cols:\n",
    "                                                i += 1; new_col_candidate = f\"{col}_{i}\"\n",
    "                                                current_col_name = new_col_candidate\n",
    "                                            new_cols.append(current_col_name); seen_cols.add(current_col_name)\n",
    "                                        combined.columns = new_cols\n",
    "                                        current_city_combined_list.append(combined.copy())\n",
    "                                        combined_loaded = True; logging.info(f\"Loaded existing combined: {combined_filepath}\")\n",
    "                                        if not os.path.exists(missing_filepath): # Ensure missing exists\n",
    "                                            missing_schema = {'geometry': 'Point', 'properties': OrderedDict([('id', 'int')])}\n",
    "                                            empty_missing = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                            try:\n",
    "                                                empty_missing.to_file(missing_filepath, driver=\"GeoJSON\", schema=missing_schema)\n",
    "                                            except Exception as e:\n",
    "                                                pass\n",
    "                                        continue\n",
    "                                    except (FileNotFoundError, DriverError): logging.info(f\"Combined not found: {combined_filepath}.\")\n",
    "                                    except Exception as e_lc: logging.error(f\"Error loading combined: {e_lc}\")\n",
    "\n",
    "                                    if not combined_loaded:\n",
    "                                        skip_download_filter = False; osm_data = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS); filtered_mapillary_data = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                                        try: osm_data = gpd.read_file(osm_processed_filepath); filtered_mapillary_data = gpd.read_file(mapillary_processed_filepath); skip_download_filter = True; logging.info(\"Pre-loaded files.\")\n",
    "                                        except (FileNotFoundError, DriverError): logging.info(\"Pre-loading failed.\")\n",
    "                                        except Exception as e_pre: logging.error(f\"Preload error: {e_pre}\")\n",
    "\n",
    "                                        if not skip_download_filter:\n",
    "                                               if filter_type == 'osm':\n",
    "                                                    osm_tags_to_download = {filter_value}\n",
    "                                                    osm_data = load_data(osm_processed_filepath, osm_raw_filepath, download_osm_data, osm_tags_to_download, area_gdf.geometry[0])\n",
    "                                                    # --- Save processed OSM data ---\n",
    "                                                    if not osm_data.empty:\n",
    "                                                         osm_data_to_save = convert_lists_to_strings(osm_data.copy())\n",
    "                                                         if not osm_data_to_save.geometry.is_valid.all(): # Validation\n",
    "                                                             try:\n",
    "                                                                 orig_crs_osm = osm_data_to_save.crs\n",
    "                                                                 osm_data_to_save.geometry = osm_data_to_save.geometry.buffer(0)\n",
    "                                                                 osm_data_to_save = osm_data_to_save[~osm_data_to_save.geometry.is_empty]\n",
    "                                                                 if osm_data_to_save.empty:\n",
    "                                                                     logging.warning(f\"All proc OSM geoms empty after buffer for {filter_value}.\")\n",
    "                                                                 elif osm_data_to_save.crs != orig_crs_osm:\n",
    "                                                                     osm_data_to_save = osm_data_to_save.set_crs(orig_crs_osm, allow_override=True)\n",
    "                                                             except Exception as buffer_err_osm:\n",
    "                                                                 logging.error(f\"Error OSM buffer: {buffer_err_osm}\")\n",
    "                                                                 osm_data_to_save = convert_lists_to_strings(osm_data.copy()) # Revert\n",
    "                                                         if not osm_data_to_save.empty: # Save if not empty\n",
    "                                                              try:\n",
    "                                                                  osm_data_to_save.to_file(osm_processed_filepath, driver=\"GeoJSON\")\n",
    "                                                                  logging.info(f\"Saved proc OSM: {osm_processed_filepath}\")\n",
    "                                                              except Exception as save_osm_err:\n",
    "                                                                  logging.error(f\"Fail save proc OSM: {save_osm_err}\")\n",
    "                                                                  osm_data = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                                                         else: # Save empty (buffer)\n",
    "                                                              empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                              schema = create_schema(empty_gdf)\n",
    "                                                              try:\n",
    "                                                                  empty_gdf.to_file(osm_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                              except Exception as e_seb:\n",
    "                                                                  logging.error(f\"Failed save empty proc OSM (buffer empty): {e_seb}\")\n",
    "                                                    else: # Save empty (download)\n",
    "                                                         empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                         schema = create_schema(empty_gdf)\n",
    "                                                         try:\n",
    "                                                             empty_gdf.to_file(osm_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                         except Exception as e_sed:\n",
    "                                                             logging.error(f\"Failed save empty proc OSM (download empty): {e_sed}\")\n",
    "                                                    # --- Determine Mapillary filters ---\n",
    "                                                    mapillary_filter_values = []\n",
    "                                                    if 'osm_tag_1' in mapillary_osm_mapping.columns and 'mapillary_feature' in mapillary_osm_mapping.columns:\n",
    "                                                         mapillary_filter_values.extend(mapillary_osm_mapping[mapillary_osm_mapping['osm_tag_1'] == filter_value]['mapillary_feature'].dropna().tolist())\n",
    "                                                    if country_code and country_code in country_mappings:\n",
    "                                                         country_df_map = country_mappings[country_code]\n",
    "                                                         if 'mapillary_feature' in country_df_map.columns:\n",
    "                                                             for _, row in country_df_map.iterrows():\n",
    "                                                                 for osm_col in OSM_TAG_COLS:\n",
    "                                                                     if osm_col in row and pd.notna(row[osm_col]) and row[osm_col] == filter_value:\n",
    "                                                                         mapillary_feature = str(row['mapillary_feature'])\n",
    "                                                                         if pd.notna(mapillary_feature) and mapillary_feature != '':\n",
    "                                                                             mapillary_filter_values.append(mapillary_feature)\n",
    "                                                                         break\n",
    "                                                    mapillary_filter_values = [str(x) for x in set(mapillary_filter_values) if x is not None and x != '']\n",
    "                                                    processed_mapillary_filters.update(mapillary_filter_values)\n",
    "                                                    # --- Load/Download/Process Mapillary ---\n",
    "                                                    if mapillary_filter_values:\n",
    "                                                         mapillary_data_gdf = load_data(mapillary_processed_filepath, mapillary_raw_filepath, download_mapillary_data, bbox, mapillary_filter_values, country_code, country_mappings, existed_at, existed_before)\n",
    "                                                         filtered_mapillary_data = filter_mapillary_data(mapillary_data_gdf, area_gdf)\n",
    "                                                         logging.info(f\"Spatially filtered Mply {filter_value}: {len(filtered_mapillary_data)} features\")\n",
    "                                                         # <<< Block 1: Mply Tag Merge/Filter/Accumulate/Save >>>\n",
    "                                                         if not filtered_mapillary_data.empty:\n",
    "                                                              if 'mapillary_feature' not in filtered_mapillary_data.columns:\n",
    "                                                                  if 'value' in filtered_mapillary_data.columns:\n",
    "                                                                      filtered_mapillary_data['mapillary_feature'] = filtered_mapillary_data['value'].astype(str).str.strip()\n",
    "                                                                  else:\n",
    "                                                                      filtered_mapillary_data = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                                                              else:\n",
    "                                                                  filtered_mapillary_data['mapillary_feature'] = filtered_mapillary_data['mapillary_feature'].astype(str).str.strip()\n",
    "                                                              if not filtered_mapillary_data.empty: # Continue processing\n",
    "                                                                  mapillary_osm_mapping_clean = mapillary_osm_mapping.copy()\n",
    "                                                                  mapillary_osm_mapping_clean['mapillary_feature'] = mapillary_osm_mapping_clean['mapillary_feature'].astype(str).str.strip()\n",
    "                                                                  global_mapping_cols = ['mapillary_feature'] + [col for col in OSM_TAG_COLS if col in mapillary_osm_mapping_clean.columns]\n",
    "                                                                  filtered_mapillary_data = filtered_mapillary_data.merge( mapillary_osm_mapping_clean[global_mapping_cols].drop_duplicates(subset=['mapillary_feature']), on='mapillary_feature', how='left' )\n",
    "                                                                  country_merge_happened = False\n",
    "                                                                  if country_code and country_code in country_mappings:\n",
    "                                                                      country_df = country_mappings[country_code].copy()\n",
    "                                                                      country_df['mapillary_feature'] = country_df['mapillary_feature'].astype(str).str.strip()\n",
    "                                                                      country_mapping_cols = ['mapillary_feature'] + [col for col in OSM_TAG_COLS if col in country_df.columns]\n",
    "                                                                      if 'country_tag' in country_df.columns:\n",
    "                                                                          country_mapping_cols.append('country_tag')\n",
    "                                                                      filtered_mapillary_data = filtered_mapillary_data.merge( country_df[list(set(country_mapping_cols))].drop_duplicates(subset=['mapillary_feature']), on='mapillary_feature', how='left', suffixes=('', '_country') )\n",
    "                                                                      country_merge_happened = True\n",
    "                                                                  else:\n",
    "                                                                      logging.warning(f\"Block 1 - No country map for {country_code}\")\n",
    "                                                                  if country_merge_happened: # Conflict resolution\n",
    "                                                                      for osm_col in OSM_TAG_COLS:\n",
    "                                                                          suffixed_col = f\"{osm_col}_country\"\n",
    "                                                                          if suffixed_col in filtered_mapillary_data.columns:\n",
    "                                                                              filtered_mapillary_data[osm_col] = filtered_mapillary_data[osm_col].fillna(filtered_mapillary_data[suffixed_col])\n",
    "                                                                              filtered_mapillary_data = filtered_mapillary_data.drop(columns=[suffixed_col], errors='ignore')\n",
    "                                                                  filtered_mapillary_data = filtered_mapillary_data.loc[:, ~filtered_mapillary_data.columns.duplicated()] # Dedupe cols\n",
    "                                                                  cols_to_keep = ['geometry', 'mapillary_feature'] + OSM_TAG_COLS + ['country_tag']\n",
    "                                                                  cols_to_keep_actual = [c for c in cols_to_keep if c in filtered_mapillary_data.columns] # Select cols\n",
    "                                                                  if not cols_to_keep_actual:\n",
    "                                                                      filtered_mapillary_data = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                                                                  else: # Clean cols\n",
    "                                                                       filtered_mapillary_data = filtered_mapillary_data[cols_to_keep_actual]\n",
    "                                                                       tag_columns_present = [col for col in OSM_TAG_COLS if col in filtered_mapillary_data.columns] + (['country_tag'] if 'country_tag' in filtered_mapillary_data.columns else [])\n",
    "                                                                       for col in tag_columns_present:\n",
    "                                                                            if pd.api.types.is_object_dtype(filtered_mapillary_data[col]):\n",
    "                                                                                filtered_mapillary_data[col] = filtered_mapillary_data[col].astype(str).replace('nan', '').fillna('')\n",
    "                                                                            else:\n",
    "                                                                                filtered_mapillary_data[col] = filtered_mapillary_data[col].fillna('')\n",
    "                                                              # CRS Check/Enforce UTM\n",
    "                                                              if not filtered_mapillary_data.empty and filtered_mapillary_data.crs != UTM_CRS:\n",
    "                                                                  try:\n",
    "                                                                      filtered_mapillary_data = filtered_mapillary_data.to_crs(UTM_CRS)\n",
    "                                                                  except Exception as e_crs_b1:\n",
    "                                                                      logging.error(f\"Block 1 CRS Fail: {e_crs_b1}\")\n",
    "                                                                      filtered_mapillary_data = gpd.GeoDataFrame()\n",
    "                                                              # --- country_tag Filter Block 1 REMOVED ---\n",
    "                                                         # Accumulate for city\n",
    "                                                         if not filtered_mapillary_data.empty:\n",
    "                                                              if 'source' not in filtered_mapillary_data.columns:\n",
    "                                                                  filtered_mapillary_data['source'] = 'Mapillary'\n",
    "                                                              city_mapillary_signs_processed_list.append(filtered_mapillary_data.copy())\n",
    "                                                         # Save processed Mply for this filter\n",
    "                                                         if not filtered_mapillary_data.empty:\n",
    "                                                             try:\n",
    "                                                                 if 'geometry' in filtered_mapillary_data.columns and filtered_mapillary_data.geometry.name != 'geometry':\n",
    "                                                                     filtered_mapillary_data = filtered_mapillary_data.set_geometry('geometry')\n",
    "                                                                 filtered_mapillary_data.to_file(mapillary_processed_filepath, driver=\"GeoJSON\")\n",
    "                                                                 logging.info(f\"Block 1 - Saved proc Mply: {mapillary_processed_filepath}\")\n",
    "                                                             except Exception as save_map_err:\n",
    "                                                                 logging.error(f\"Block 1 - Fail save proc Mply: {save_map_err}\")\n",
    "                                                                 empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                                 schema = create_schema(empty_gdf)\n",
    "                                                                 try:\n",
    "                                                                     empty_gdf.to_file(mapillary_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                                 except Exception as e_fb_save:\n",
    "                                                                     logging.error(f\"Failed fallback save: {e_fb_save}\")\n",
    "                                                         else: # Save empty\n",
    "                                                              empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                              schema = create_schema(empty_gdf)\n",
    "                                                              try:\n",
    "                                                                  empty_gdf.to_file(mapillary_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                              except Exception as e_save_empty_mply1:\n",
    "                                                                  logging.error(f\"Failed save empty Mply (filter empty): {e_save_empty_mply1}\")\n",
    "                                                    else: # No Mply filters\n",
    "                                                         filtered_mapillary_data = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                                                         empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                         schema = create_schema(empty_gdf)\n",
    "                                                         try:\n",
    "                                                             empty_gdf.to_file(mapillary_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                         except Exception as e_save_empty_mply_nf:\n",
    "                                                             logging.error(f\"Failed save empty Mply (no filters): {e_save_empty_mply_nf}\")\n",
    "                                                    # --- End Mapillary ---\n",
    "\n",
    "                                        # --- Conflation Stage (Main Combined/Missing files) ---\n",
    "                                        start_time = time.time()\n",
    "                                        df1 = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS); df2 = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                                        if isinstance(osm_data, gpd.GeoDataFrame) and not osm_data.empty:\n",
    "                                            try:\n",
    "                                                df1 = osm_data.to_crs(UTM_CRS) if osm_data.crs != UTM_CRS else osm_data.copy()\n",
    "                                            except Exception as e_p1:\n",
    "                                                logging.error(f\"Error projecting df1 for conflation: {e_p1}\")\n",
    "                                        if isinstance(filtered_mapillary_data, gpd.GeoDataFrame) and not filtered_mapillary_data.empty:\n",
    "                                            try:\n",
    "                                                df2 = filtered_mapillary_data.to_crs(UTM_CRS) if filtered_mapillary_data.crs != UTM_CRS else filtered_mapillary_data.copy()\n",
    "                                            except Exception as e_p2:\n",
    "                                                logging.error(f\"Error projecting df2 for conflation: {e_p2}\")\n",
    "\n",
    "                                        combined = None; missing = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS); overlap = gpd.GeoDataFrame(); properties_to_keep_conflation = ['mapillary_feature'] + OSM_TAG_COLS + ['country_tag', 'source']\n",
    "                                        logging.info(f\"----- Conflation Check (Main Combined/Missing) Filter: {filter_value} -----\")\n",
    "                                        logging.info(f\"df1 (OSM) empty: {df1.empty}, df2 (Mply) empty: {df2.empty}\")\n",
    "                                        # Perform Conflation...\n",
    "                                        if not df1.empty and not df2.empty:\n",
    "                                            if transport_agency_data_exists:\n",
    "                                                agency_df_proj = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "                                                try:\n",
    "                                                    if isinstance(transport_agency_osm_df, gpd.GeoDataFrame) and not transport_agency_osm_df.empty:\n",
    "                                                        agency_df_proj = transport_agency_osm_df.to_crs(UTM_CRS) if transport_agency_osm_df.crs != UTM_CRS else transport_agency_osm_df.copy()\n",
    "                                                except Exception as e_pa:\n",
    "                                                    logging.error(f\"Error projecting agency_df: {e_pa}\")\n",
    "                                                if not agency_df_proj.empty:\n",
    "                                                    base_df, add1_df, add2_df = agency_df_proj.copy(), df1.copy(), df2.copy()\n",
    "                                                    osm_mapper_flag = False\n",
    "                                                    combined, _, _, _, overlap, missing = analyze_overlap_3dfs( base_df, add1_df, add2_df, properties_to_keep=properties_to_keep_conflation, osm_mapper_mode=osm_mapper_flag, buffer_distance=BUFFER_DISTANCE)\n",
    "                                                else:\n",
    "                                                    base_df, add_df = df1.copy(), df2.copy()\n",
    "                                                    combined, overlap, missing = analyze_overlap_2dfs( base_df, add_df, properties_to_keep=properties_to_keep_conflation, buffer_distance=BUFFER_DISTANCE)\n",
    "                                            else:\n",
    "                                                base_df, add_df = df1.copy(), df2.copy()\n",
    "                                                combined, overlap, missing = analyze_overlap_2dfs( base_df, add_df, properties_to_keep=properties_to_keep_conflation, buffer_distance=BUFFER_DISTANCE)\n",
    "                                        elif not df1.empty:\n",
    "                                            combined = df1.copy(); missing = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS); overlap = gpd.GeoDataFrame(); combined['source'] = combined.get('source', 'OSM')\n",
    "                                        elif not df2.empty:\n",
    "                                            combined = df2.copy(); missing = df2.copy(); overlap = gpd.GeoDataFrame(); combined['source'] = combined.get('source', 'Mapillary'); missing['source'] = missing.get('source', 'Mapillary')\n",
    "                                        else:\n",
    "                                            combined = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS); missing = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS); overlap = gpd.GeoDataFrame()\n",
    "\n",
    "                                        # --- Process and Save Conflation Results ---\n",
    "                                        if combined is not None:\n",
    "                                            # Convert/save combined...\n",
    "                                            combined_gdf, _ = convert_osm_tags_to_kv(combined.copy())\n",
    "                                            if isinstance(combined_gdf, gpd.GeoDataFrame) and not combined_gdf.empty:\n",
    "                                                if combined_gdf.crs != UTM_CRS:\n",
    "                                                    try:\n",
    "                                                        combined_gdf = combined_gdf.to_crs(UTM_CRS)\n",
    "                                                    except Exception:\n",
    "                                                        combined_gdf = gpd.GeoDataFrame()\n",
    "                                                if not combined_gdf.empty and not combined_gdf.geometry.is_valid.all(): # Validation...\n",
    "                                                    try:\n",
    "                                                        orig_crs = combined_gdf.crs\n",
    "                                                        combined_gdf.geometry = combined_gdf.geometry.buffer(0)\n",
    "                                                        combined_gdf = combined_gdf[~combined_gdf.geometry.is_empty]\n",
    "                                                        if not combined_gdf.empty and combined_gdf.crs != orig_crs:\n",
    "                                                            combined_gdf = combined_gdf.set_crs(orig_crs, allow_override=True)\n",
    "                                                    except Exception: pass\n",
    "                                                if not combined_gdf.empty: # Save if still valid\n",
    "                                                    try:\n",
    "                                                        combined_gdf_to_save = combined_gdf.drop(columns=['city_id'], errors='ignore')\n",
    "                                                        combined_gdf_to_save.to_file(combined_filepath, driver=\"GeoJSON\")\n",
    "                                                    except Exception as e:\n",
    "                                                        logging.error(f\"Failed save combined: {e}\")\n",
    "                                                    current_city_combined_list.append(combined_gdf.copy())\n",
    "                                                else: # Save empty combined\n",
    "                                                    empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                    schema = create_schema(empty_gdf)\n",
    "                                                    try:\n",
    "                                                        empty_gdf.to_file(combined_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                    except Exception: pass\n",
    "                                            else: # Save empty combined\n",
    "                                                empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                schema = create_schema(empty_gdf)\n",
    "                                                try:\n",
    "                                                    empty_gdf.to_file(combined_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                except Exception: pass\n",
    "                                            # Convert/save missing...\n",
    "                                            missing_gdf, _ = convert_osm_tags_to_kv(missing.copy())\n",
    "                                            if isinstance(missing_gdf, gpd.GeoDataFrame) and not missing_gdf.empty:\n",
    "                                                missing_gdf = missing_gdf.dropna(axis=1, how='all')\n",
    "                                                if missing_gdf.crs != UTM_CRS:\n",
    "                                                    try:\n",
    "                                                        missing_gdf = missing_gdf.to_crs(UTM_CRS)\n",
    "                                                    except Exception:\n",
    "                                                        missing_gdf = gpd.GeoDataFrame()\n",
    "                                                if not missing_gdf.empty and not missing_gdf.geometry.is_valid.all(): # Validation...\n",
    "                                                    try:\n",
    "                                                        orig_crs = missing_gdf.crs\n",
    "                                                        missing_gdf.geometry = missing_gdf.geometry.buffer(0)\n",
    "                                                        missing_gdf = missing_gdf[~missing_gdf.geometry.is_empty]\n",
    "                                                        if not missing_gdf.empty and missing_gdf.crs != orig_crs:\n",
    "                                                            missing_gdf = missing_gdf.set_crs(orig_crs, allow_override=True)\n",
    "                                                    except Exception: pass\n",
    "                                                if not missing_gdf.empty: # Save if still valid\n",
    "                                                    try:\n",
    "                                                        missing_gdf_to_save = missing_gdf.drop(columns=['city_id'], errors='ignore')\n",
    "                                                        missing_gdf_to_save.to_file(missing_filepath, driver=\"GeoJSON\")\n",
    "                                                    except Exception as e: logging.error(f\"Failed save missing: {e}\")\n",
    "                                                else: # Save empty missing\n",
    "                                                    empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                    schema = create_schema(empty_gdf)\n",
    "                                                    try:\n",
    "                                                        empty_gdf.to_file(missing_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                    except Exception: pass\n",
    "                                            else: # Save empty missing\n",
    "                                                empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                                schema = create_schema(empty_gdf)\n",
    "                                                try:\n",
    "                                                    empty_gdf.to_file(missing_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                                except Exception: pass\n",
    "                                        else: logging.error(f\"Combined df None for {filter_value}.\")\n",
    "\n",
    "                                    # --- End Filter Processing ('if not combined_loaded:') ---\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"A major error occurred during the data loading/generation phase for city '{city_id}': {e}\", exc_info=True)\n",
    "                    continue\n",
    "            # --- End Filters Loop --- ('for filter_value in filter_values:')\n",
    "        # --- End Filter Type Loop --- ('for filter_type, ...')\n",
    "\n",
    "        # --- Download and Process Remaining Mapillary Traffic Signs ---\n",
    "        if not processed_network_loaded:\n",
    "            all_traffic_sign_mapillary_features = []\n",
    "            processed_mapillary_filters_set = set(processed_mapillary_filters)\n",
    "            for code, df_country in country_mappings.items():\n",
    "                if 'mapillary_feature' in df_country.columns:\n",
    "                    all_traffic_sign_mapillary_features.extend(df_country['mapillary_feature'].dropna().astype(str).str.strip().tolist())\n",
    "            all_traffic_sign_mapillary_features = list(set(all_traffic_sign_mapillary_features) - processed_mapillary_filters_set)\n",
    "            all_traffic_sign_mapillary_features = [x for x in all_traffic_sign_mapillary_features if x]\n",
    "\n",
    "            mapillary_traffic_signs_raw_filepath = os.path.join(DATA_DIR, city_id, \"raw\", f\"Mapillary_raw_{city_id}_traffic_signs.geojson\")\n",
    "            mapillary_traffic_signs_processed_filepath = os.path.join(DATA_DIR, city_id, \"Mapillary\", f\"Mapillary_{city_id}_traffic_signs.geojson\")\n",
    "\n",
    "            if 'area_gdf' not in locals():\n",
    "                try:\n",
    "                    area_gdf, bbox = get_area_from_input(user_input, city_id)\n",
    "                except Exception as e:\n",
    "                    bbox = None\n",
    "                    area_gdf = None\n",
    "            if bbox and area_gdf is not None and all_traffic_sign_mapillary_features:\n",
    "                logging.info(f\"Processing {len(all_traffic_sign_mapillary_features)} remaining Mply traffic signs...\")\n",
    "                filtered_mapillary_traffic_signs = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                loaded_remaining = False\n",
    "                try: # Try loading existing\n",
    "                    filtered_mapillary_traffic_signs = gpd.read_file(mapillary_traffic_signs_processed_filepath)\n",
    "                    loaded_remaining=True\n",
    "                    logging.info(f\"Loaded existing remaining signs.\")\n",
    "                    if filtered_mapillary_traffic_signs.crs != UTM_CRS:\n",
    "                        filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.to_crs(UTM_CRS)\n",
    "                except: # Download if load fails\n",
    "                    logging.info(f\"Remaining TS file not found, downloading.\")\n",
    "                    mapillary_traffic_signs_gdf = load_data(mapillary_traffic_signs_processed_filepath, mapillary_traffic_signs_raw_filepath, download_mapillary_data, bbox, all_traffic_sign_mapillary_features, country_code, country_mappings, existed_at, existed_before )\n",
    "                    filtered_mapillary_traffic_signs = filter_mapillary_data(mapillary_traffic_signs_gdf, area_gdf)\n",
    "                    # Process ONLY if downloaded/filtered\n",
    "                    # <<< START: Mapillary Tag Merging & Filtering Block 2 >>>\n",
    "                    if not filtered_mapillary_traffic_signs.empty:\n",
    "                            if 'mapillary_feature' not in filtered_mapillary_traffic_signs.columns:\n",
    "                                if 'value' in filtered_mapillary_traffic_signs.columns:\n",
    "                                    filtered_mapillary_traffic_signs['mapillary_feature'] = filtered_mapillary_traffic_signs['value'].astype(str).str.strip()\n",
    "                                else:\n",
    "                                    filtered_mapillary_traffic_signs = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                            else:\n",
    "                                filtered_mapillary_traffic_signs['mapillary_feature'] = filtered_mapillary_traffic_signs['mapillary_feature'].astype(str).str.strip()\n",
    "                            if not filtered_mapillary_traffic_signs.empty:\n",
    "                                mapillary_osm_mapping_clean = mapillary_osm_mapping.copy()\n",
    "                                mapillary_osm_mapping_clean['mapillary_feature'] = mapillary_osm_mapping_clean['mapillary_feature'].astype(str).str.strip()\n",
    "                                global_mapping_cols = ['mapillary_feature'] + [col for col in OSM_TAG_COLS if col in mapillary_osm_mapping_clean.columns]\n",
    "                                filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.merge( mapillary_osm_mapping_clean[global_mapping_cols].drop_duplicates(subset=['mapillary_feature']), on='mapillary_feature', how='left' )\n",
    "                                country_merge_happened_b2 = False\n",
    "                                if country_code and country_code in country_mappings:\n",
    "                                    country_df = country_mappings[country_code].copy()\n",
    "                                    country_df['mapillary_feature'] = country_df['mapillary_feature'].astype(str).str.strip()\n",
    "                                    country_mapping_cols = ['mapillary_feature'] + [col for col in OSM_TAG_COLS if col in country_df.columns]\n",
    "                                    if 'country_tag' in country_df.columns:\n",
    "                                        country_mapping_cols.append('country_tag')\n",
    "                                    filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.merge( country_df[list(set(country_mapping_cols))].drop_duplicates(subset=['mapillary_feature']), on='mapillary_feature', how='left', suffixes=('', '_country') )\n",
    "                                    country_merge_happened_b2 = True\n",
    "                                if country_merge_happened_b2:\n",
    "                                    for osm_col in OSM_TAG_COLS:\n",
    "                                        suffixed_col = f\"{osm_col}_country\"\n",
    "                                        if suffixed_col in filtered_mapillary_traffic_signs.columns:\n",
    "                                            filtered_mapillary_traffic_signs[osm_col] = filtered_mapillary_traffic_signs[osm_col].fillna(filtered_mapillary_traffic_signs[suffixed_col])\n",
    "                                            filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.drop(columns=[suffixed_col], errors='ignore')\n",
    "                                filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.loc[:, ~filtered_mapillary_traffic_signs.columns.duplicated()]\n",
    "                                cols_to_keep = ['geometry', 'mapillary_feature'] + OSM_TAG_COLS + ['country_tag']\n",
    "                                cols_to_keep_actual = [c for c in cols_to_keep if c in filtered_mapillary_traffic_signs.columns]\n",
    "                                if not cols_to_keep_actual:\n",
    "                                    filtered_mapillary_traffic_signs = gpd.GeoDataFrame(geometry=[], crs=UTM_CRS)\n",
    "                                else:\n",
    "                                    filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs[cols_to_keep_actual]\n",
    "                                    tag_columns_present = [col for col in OSM_TAG_COLS if col in filtered_mapillary_traffic_signs.columns] + (['country_tag'] if 'country_tag' in filtered_mapillary_traffic_signs.columns else [])\n",
    "                                    for col in tag_columns_present:\n",
    "                                        if pd.api.types.is_object_dtype(filtered_mapillary_traffic_signs[col]):\n",
    "                                            filtered_mapillary_traffic_signs[col] = filtered_mapillary_traffic_signs[col].astype(str).replace('nan', '').fillna('')\n",
    "                                        else:\n",
    "                                            filtered_mapillary_traffic_signs[col] = filtered_mapillary_traffic_signs[col].fillna('')\n",
    "                            # CRS Check/Enforce UTM\n",
    "                            if not filtered_mapillary_traffic_signs.empty and filtered_mapillary_traffic_signs.crs != UTM_CRS:\n",
    "                                try:\n",
    "                                    filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.to_crs(UTM_CRS)\n",
    "                                except Exception as crs_err_b2:\n",
    "                                    logging.error(f\"Block 2 - Failed re-projection: {crs_err_b2}\")\n",
    "                                    filtered_mapillary_traffic_signs = gpd.GeoDataFrame()\n",
    "                    # <<< END: Mapillary Tag Merging & Filtering Block 2 >>>\n",
    "\n",
    "                # --- Accumulate remaining Mapillary data ---\n",
    "                if not filtered_mapillary_traffic_signs.empty:\n",
    "                        if 'source' not in filtered_mapillary_traffic_signs.columns:\n",
    "                            filtered_mapillary_traffic_signs['source'] = 'Mapillary'\n",
    "                        if filtered_mapillary_traffic_signs.crs != UTM_CRS:\n",
    "                            try:\n",
    "                                filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.to_crs(UTM_CRS)\n",
    "                            except Exception:\n",
    "                                pass # If conversion fails, log below\n",
    "                        if filtered_mapillary_traffic_signs.crs == UTM_CRS:\n",
    "                            city_mapillary_signs_processed_list.append(filtered_mapillary_traffic_signs.copy())\n",
    "                        else:\n",
    "                            logging.warning(\"Skipping accumulation remaining signs: CRS issue.\")\n",
    "\n",
    "                # Save remaining processed ONLY IF GENERATED this run\n",
    "                if not loaded_remaining:\n",
    "                        if not filtered_mapillary_traffic_signs.empty:\n",
    "                            try:\n",
    "                                if 'geometry' in filtered_mapillary_traffic_signs.columns and filtered_mapillary_traffic_signs.geometry.name != 'geometry':\n",
    "                                    filtered_mapillary_traffic_signs = filtered_mapillary_traffic_signs.set_geometry('geometry')\n",
    "                                filtered_mapillary_traffic_signs_to_save = filtered_mapillary_traffic_signs.drop(columns=['city_id'], errors='ignore')\n",
    "                                filtered_mapillary_traffic_signs_to_save.to_file(mapillary_traffic_signs_processed_filepath, driver=\"GeoJSON\")\n",
    "                                logging.info(f\"Block 2 - Saved processed remaining signs: {mapillary_traffic_signs_processed_filepath} ({len(filtered_mapillary_traffic_signs_to_save)} features).\")\n",
    "                            except Exception as save_map_rem_err:\n",
    "                                logging.error(f\"Block 2 - Failed save remaining Mply: {save_map_rem_err}\")\n",
    "                                # Save empty fallback...\n",
    "                                empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                                schema = create_schema(empty_gdf)\n",
    "                                try:\n",
    "                                    empty_gdf.to_file(mapillary_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                                except Exception as e_fb_save_rem:\n",
    "                                    logging.error(f\"Failed fallback save remaining Mply: {e_fb_save_rem}\")\n",
    "                        else: # Save empty\n",
    "                            empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                            schema = create_schema(empty_gdf)\n",
    "                            try:\n",
    "                                empty_gdf.to_file(mapillary_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                            except Exception as e_save_empty_rem:\n",
    "                                logging.error(f\"Failed save empty remaining Mply: {e_save_empty_rem}\")\n",
    "            else: # Skip remaining signs\n",
    "                logging.warning(\"Skipping remaining traffic signs: issue with bbox/area/features.\")\n",
    "                if not os.path.exists(mapillary_traffic_signs_processed_filepath): # Ensure empty file exists\n",
    "                        empty_gdf = gpd.GeoDataFrame({'geometry': []}, geometry='geometry', crs=UTM_CRS)\n",
    "                        schema = create_schema(empty_gdf)\n",
    "                        try:\n",
    "                            empty_gdf.to_file(mapillary_traffic_signs_processed_filepath, driver=\"GeoJSON\", schema=schema)\n",
    "                        except Exception as e_save_empty_rem_skip:\n",
    "                            logging.error(f\"Failed save empty remaining signs (skipped): {e_save_empty_rem_skip}\")\n",
    "\n",
    "        # --- Consolidate accumulated Mapillary signs for the city ---\n",
    "        all_city_mapillary_signs_processed = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "        if city_mapillary_signs_processed_list:\n",
    "            try:\n",
    "                valid_parts = [gdf for gdf in city_mapillary_signs_processed_list if isinstance(gdf, gpd.GeoDataFrame) and 'geometry' in gdf.columns and not gdf.empty]\n",
    "                if valid_parts:\n",
    "                    all_city_mapillary_signs_processed = pd.concat(valid_parts, ignore_index=True, sort=False)\n",
    "                    if not isinstance(all_city_mapillary_signs_processed, gpd.GeoDataFrame):\n",
    "                        all_city_mapillary_signs_processed = gpd.GeoDataFrame(all_city_mapillary_signs_processed, geometry='geometry', crs=UTM_CRS)\n",
    "                    elif all_city_mapillary_signs_processed.crs != UTM_CRS:\n",
    "                        all_city_mapillary_signs_processed = all_city_mapillary_signs_processed.to_crs(UTM_CRS)\n",
    "                    # Initial Deduplication...\n",
    "                    if not all_city_mapillary_signs_processed.empty and 'mapillary_feature' in all_city_mapillary_signs_processed.columns and 'geometry' in all_city_mapillary_signs_processed.columns:\n",
    "                        all_city_mapillary_signs_processed['geom_wkt_temp'] = all_city_mapillary_signs_processed.geometry.to_wkt()\n",
    "                        initial_count = len(all_city_mapillary_signs_processed)\n",
    "                        all_city_mapillary_signs_processed.drop_duplicates(subset=['mapillary_feature', 'geom_wkt_temp'], keep='first', inplace=True)\n",
    "                        all_city_mapillary_signs_processed.drop(columns=['geom_wkt_temp'], inplace=True)\n",
    "                        if initial_count > len(all_city_mapillary_signs_processed):\n",
    "                            logging.info(f\"Removed {initial_count - len(all_city_mapillary_signs_processed)} duplicate Mply signs.\")\n",
    "                    # Spatial Deduplication...\n",
    "                    if not all_city_mapillary_signs_processed.empty and 'traffic_sign' in all_city_mapillary_signs_processed.columns:\n",
    "                        signs_to_dedupe = all_city_mapillary_signs_processed.copy()\n",
    "                        if signs_to_dedupe.crs != UTM_CRS:\n",
    "                            try:\n",
    "                                signs_to_dedupe = signs_to_dedupe.to_crs(UTM_CRS)\n",
    "                            except Exception:\n",
    "                                signs_to_dedupe = gpd.GeoDataFrame() # Make empty if projection fails\n",
    "                        if not signs_to_dedupe.empty:\n",
    "                                signs_buffered = signs_to_dedupe.copy()\n",
    "                                signs_buffered['geometry'] = signs_buffered.geometry.buffer(BUFFER_DISTANCE)\n",
    "                                nearby_pairs = gpd.sjoin(signs_to_dedupe, signs_buffered, how='inner', predicate='within', lsuffix='left', rsuffix='right')\n",
    "                                nearby_pairs = nearby_pairs[nearby_pairs.index != nearby_pairs['index_right']]\n",
    "                                nearby_pairs['traffic_sign_left'] = nearby_pairs.index.map(signs_to_dedupe['traffic_sign'])\n",
    "                                nearby_pairs['traffic_sign_right'] = nearby_pairs['index_right'].map(signs_to_dedupe['traffic_sign'])\n",
    "                                identical_nearby = nearby_pairs[(nearby_pairs['traffic_sign_left'] == nearby_pairs['traffic_sign_right']) & (nearby_pairs['traffic_sign_left'] != '')]\n",
    "                                indices_to_drop = set()\n",
    "                                if not identical_nearby.empty:\n",
    "                                    indices_higher = identical_nearby.apply(lambda row: max(row.name, row['index_right']), axis=1)\n",
    "                                    indices_to_drop.update(indices_higher.tolist())\n",
    "                                if indices_to_drop:\n",
    "                                    count_before = len(all_city_mapillary_signs_processed)\n",
    "                                    all_city_mapillary_signs_processed.drop(index=list(indices_to_drop), inplace=True)\n",
    "                                    logging.info(f\"Removed {count_before - len(all_city_mapillary_signs_processed)} nearby identical Mply signs.\")\n",
    "                else:\n",
    "                    logging.warning(\"No valid GDFs to consolidate Mply signs.\")\n",
    "            except Exception as concat_err:\n",
    "                logging.error(f\"Error consolidating/dedup Mply signs: {concat_err}\")\n",
    "                all_city_mapillary_signs_processed = gpd.GeoDataFrame(crs=UTM_CRS) # Reset\n",
    "        logging.info(f\"Consolidated {len(all_city_mapillary_signs_processed)} unique Mply signs for {city_id}.\")\n",
    "\n",
    "        # --- Network Processing ---\n",
    "        logging.info(\"Starting network processing...\")\n",
    "        current_city_combined = gpd.GeoDataFrame(crs=UTM_CRS)\n",
    "        if 'current_city_combined_list' in locals() and current_city_combined_list:\n",
    "            valid_parts = [gdf for gdf in current_city_combined_list if isinstance(gdf, gpd.GeoDataFrame) and not gdf.empty]\n",
    "            if valid_parts:\n",
    "                try:\n",
    "                    current_city_combined = pd.concat(valid_parts, ignore_index=True, sort=False)\n",
    "                    if not isinstance(current_city_combined, gpd.GeoDataFrame):\n",
    "                        current_city_combined = gpd.GeoDataFrame(current_city_combined, geometry='geometry', crs=UTM_CRS)\n",
    "                    if current_city_combined.crs != UTM_CRS:\n",
    "                        current_city_combined = current_city_combined.to_crs(UTM_CRS)\n",
    "                except Exception as e_concat_city:\n",
    "                    logging.error(f\"Error concatenating combined data for {city_id}: {e_concat_city}\")\n",
    "                    current_city_combined = gpd.GeoDataFrame(crs=UTM_CRS) # Ensure it's empty on failure\n",
    "        else:\n",
    "             if not processed_network_loaded: # Only warn if we were actually generating the network\n",
    "                logging.warning(f\"No combined data was loaded or generated for {city_id}. Way tag processing will be limited.\")\n",
    "\n",
    "        try: # Process network\n",
    "            if 'boundary_polygon' not in locals(): boundary_polygon = area_gdf.geometry.iloc[0]\n",
    "            if boundary_polygon is None: raise ValueError(\"Boundary polygon missing for graph download\")\n",
    "\n",
    "            gdf_nodes_proj, gdf_edges, G_projected = download_and_process_osm_graph(boundary_polygon) # Get projected graph\n",
    "            if G_projected is None or gdf_edges.empty: raise ValueError(\"Graph download/projection failed\")\n",
    "\n",
    "            # --- Call processing functions ---\n",
    "            gdf_edges = process_cycleways_and_busways(gdf_edges)\n",
    "            if gdf_edges.crs != UTM_CRS: gdf_edges = gdf_edges.to_crs(UTM_CRS) # Ensure edges are UTM\n",
    "\n",
    "            if isinstance(current_city_combined, gpd.GeoDataFrame) and not current_city_combined.empty:\n",
    "                gdf_edges = process_way_tags(gdf_edges, current_city_combined) # Crossings\n",
    "            else:\n",
    "                logging.warning(\"Cannot process way tags (crossings); input data invalid/empty.\")\n",
    "                for item_value_cross in way_tags.get('crossing', []): gdf_edges[f\"crossing_{item_value_cross}_count\"] = 0\n",
    "            gdf_edges = process_traffic_signs(gdf_edges, user_input, current_city_combined, all_city_mapillary_signs_processed, city_id, DATA_DIR, G_projected, boundary_polygon)\n",
    "\n",
    "            # --- Final Column Selection and Cleanup ---\n",
    "            final_columns_to_keep = [\n",
    "                'geometry', 'highway', 'length', 'lanes', 'maxspeed', 'surface',\n",
    "                'cycle_lane', 'shared_cycle', 'bus_lane',\n",
    "                'crossing_uncontrolled_count', 'crossing_unmarked_count', 'crossing_traffic_signals_count',\n",
    "                'highway_traffic_signals_count', 'highway_stop_count', 'highway_give_way_count',\n",
    "                'traffic_sign_count'\n",
    "            ] # REMOVED city_id and all internal OSMnx IDs\n",
    "            actual_columns = [col for col in final_columns_to_keep if col in gdf_edges.columns]\n",
    "            gdf_edges = gdf_edges[actual_columns].copy()\n",
    "            if 'surface' in gdf_edges.columns: gdf_edges['surface'] = gdf_edges['surface'].fillna('unknown')\n",
    "            if 'maxspeed' in gdf_edges.columns: gdf_edges['maxspeed'] = gdf_edges['maxspeed'].fillna('')\n",
    "            if 'lanes' in gdf_edges.columns: gdf_edges['lanes'] = gdf_edges['lanes'].fillna('').astype(str).replace(['nan', 'None', 'NULL'], '', regex=False)\n",
    "            count_cols_final = [col for col in gdf_edges.columns if '_count' in col]\n",
    "            for ccol in count_cols_final:\n",
    "                if ccol in gdf_edges.columns:\n",
    "                    gdf_edges[ccol] = gdf_edges[ccol].fillna(0)\n",
    "                    try:\n",
    "                        gdf_edges[ccol] = gdf_edges[ccol].astype(np.int64)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            gdf_edges = convert_lists_to_strings(gdf_edges)\n",
    "            # --- End Final Column Selection ---\n",
    "\n",
    "            # Save network\n",
    "            try:\n",
    "                logging.debug(f\"Columns JUST BEFORE saving network GPKG: {gdf_edges.columns.tolist()}\")\n",
    "                gdf_edges.to_file(processed_network_filepath, driver=\"GPKG\")\n",
    "                logging.info(f\"Saved processed network: {processed_network_filepath}\")\n",
    "            except Exception as save_net_err:\n",
    "                logging.error(f\"!!! Failed to save processed network: {save_net_err}\", exc_info=True)\n",
    "\n",
    "        except Exception as net_proc_err:\n",
    "            logging.error(f\"Error during network processing steps for {city_id}: {net_proc_err}\", exc_info=True)\n",
    "\n",
    "        # --- Calculate and Log City Processing Time ---\n",
    "        # This runs whether network was loaded or generated (if successful)\n",
    "        if processed_network_loaded or ('gdf_edges' in locals() and gdf_edges is not None and not gdf_edges.empty):\n",
    "            end_city_time = time.time(); elapsed_city_time = end_city_time - start_city_time\n",
    "            logging.info(f\"--- Finished city {city_id} in {elapsed_city_time:.2f} sec ---\")\n",
    "            city_times_list.append({'city_id': city_id, 'user_input': user_input, 'processing_time_seconds': round(elapsed_city_time, 2)})\n",
    "            processed_city_ids.add(city_id) # Add to processed set\n",
    "            try: pd.DataFrame(city_times_list).to_csv(city_processing_log_file, sep=';', index=False); logging.debug(f\"Saved city times.\")\n",
    "            except Exception as e_save_times: logging.warning(f\"Could not save city times: {e_save_times}\")\n",
    "        else:\n",
    "            logging.warning(f\"Skipping time log for {city_id}: processing did not complete successfully.\")\n",
    "        # ---------------------------------------------\n",
    "\n",
    "    # --- Log Overall Time ---\n",
    "    overall_end_time = time.time()\n",
    "    logging.info(f\"\\n=== Total Script Execution Time: {(overall_end_time - overall_start_time):.2f} seconds ===\")\n",
    "\n",
    "    # --- Final Save of City Times ---\n",
    "    try:\n",
    "         pd.DataFrame(city_times_list).to_csv(city_processing_log_file, sep=';', index=False)\n",
    "         logging.info(f\"Final city processing times saved to {city_processing_log_file}\")\n",
    "    except Exception as e_save_times_final:\n",
    "         logging.warning(f\"Could not save final city processing times: {e_save_times_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d372ee0-66d5-4bc4-b19e-23c774478b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optional filters\n",
    "filters = {\n",
    "    'mapillary': None,\n",
    "    'osm': ['highway=stop', 'highway=traffic_signals', 'highway=give_way', 'crossing=uncontrolled', 'crossing=traffic_signals', 'crossing=unmarked'],\n",
    "    'country_tag': None,\n",
    "    'agency_id': None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4d738bc-ff3b-4bce-b6a7-08b1eac46eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded existing processing times for 100 cities.\n",
      "INFO:root:Starting processing...\n",
      "INFO:root:\n",
      "=== Processing City: Nuku'alofa, Tonga ===\n",
      "INFO:root:City 'nukualofa' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Apia, Samoa ===\n",
      "INFO:root:City 'apia' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Honiara, Solomon Islands ===\n",
      "INFO:root:City 'honiara' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Reykjavik, Iceland ===\n",
      "INFO:root:City 'reykjavik' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Suva, Fiji ===\n",
      "INFO:root:City 'suva' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: National Capital District, Papua New Guinea ===\n",
      "INFO:root:City 'national_capital_district' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Wellington City, New Zealand ===\n",
      "INFO:root:City 'wellington_city' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Kigali, Rwanda ===\n",
      "INFO:root:City 'kigali' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Oslo, Norway ===\n",
      "INFO:root:City 'oslo' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Zurich, Switzerland ===\n",
      "INFO:root:City 'zurich' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: San José, Costa Rica ===\n",
      "INFO:root:City 'san_jose' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Auckland, New Zealand ===\n",
      "INFO:root:City 'auckland' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Sofia, Bulgaria ===\n",
      "INFO:root:City 'sofia' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Belgrade, Serbia ===\n",
      "INFO:root:City 'belgrade' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Montevideo, Uruguay ===\n",
      "INFO:root:City 'montevideo' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Calgary, Canada ===\n",
      "INFO:root:City 'calgary' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Distrito de Panamá, Panama ===\n",
      "INFO:root:City 'distrito_de_panama' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Vienna, Austria ===\n",
      "INFO:root:City 'vienna' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Dublin, Ireland ===\n",
      "INFO:root:City 'dublin' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Perth, Australia ===\n",
      "INFO:root:City 'perth' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Prague, Czech Republic ===\n",
      "INFO:root:City 'prague' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Bucharest, Romania ===\n",
      "INFO:root:City 'bucharest' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Brussels, Belgium ===\n",
      "INFO:root:City 'brussels' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: La Paz, Bolivia ===\n",
      "INFO:root:City 'la_paz' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Baku, Azerbaijan ===\n",
      "INFO:root:City 'baku' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Doha, Qatar ===\n",
      "INFO:root:City 'doha' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Beirut, Lebanon ===\n",
      "INFO:root:City 'beirut' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Amsterdam, Netherlands ===\n",
      "INFO:root:City 'amsterdam' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Brisbane, Australia ===\n",
      "INFO:root:City 'brisbane' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Vancouver, Canada ===\n",
      "INFO:root:City 'vancouver' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Ciudad de Túnez, Tunisia ===\n",
      "INFO:root:City 'ciudad_de_tunez' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Caracas, Venezuela ===\n",
      "INFO:root:City 'caracas' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Budapest, Hungary ===\n",
      "INFO:root:City 'budapest' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Tashkent, Uzbekistan ===\n",
      "INFO:root:City 'tashkent' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Kyiv, Ukraine ===\n",
      "INFO:root:City 'kyiv' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Warsaw, Poland ===\n",
      "INFO:root:City 'warsaw' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Quito, Ecuador ===\n",
      "INFO:root:City 'quito' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Dakar, Senegal ===\n",
      "INFO:root:City 'dakar' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Lusaka, Zambia ===\n",
      "INFO:root:City 'lusaka' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Dubai, United Arab Emirates ===\n",
      "INFO:root:City 'dubai' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Algiers, Algeria ===\n",
      "INFO:root:City 'algiers' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Casablanca, Morocco ===\n",
      "INFO:root:City 'casablanca' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Kampala, Uganda ===\n",
      "INFO:root:City 'kampala' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Medellín, Colombia ===\n",
      "INFO:root:City 'medellin' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Berlin, Germany ===\n",
      "INFO:root:City 'berlin' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Rome, Italy ===\n",
      "INFO:root:City 'rome' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Montreal, Canada ===\n",
      "INFO:root:City 'montreal' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Amman, Jordan ===\n",
      "INFO:root:City 'amman' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: San Francisco, United States ===\n",
      "INFO:root:City 'san_francisco' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: City of Cape Town, South Africa ===\n",
      "INFO:root:City 'city_of_cape_town' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Boston, United States ===\n",
      "INFO:root:City 'boston' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Accra, Ghana ===\n",
      "INFO:root:City 'accra' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Monterrey, Mexico ===\n",
      "INFO:root:City 'monterrey' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Nairobi, Kenya ===\n",
      "INFO:root:City 'nairobi' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Melbourne, Australia ===\n",
      "INFO:root:City 'melbourne' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Guadalajara, Mexico ===\n",
      "INFO:root:City 'guadalajara' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Sydney, Australia ===\n",
      "INFO:root:City 'sydney' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Alexandria, Egypt ===\n",
      "INFO:root:City 'alexandria' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Colombo, Sri Lanka ===\n",
      "INFO:root:City 'colombo' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Yangon, Myanmar ===\n",
      "INFO:root:City 'yangon' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Singapore, Singapore ===\n",
      "INFO:root:City 'singapore' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Miami, United States ===\n",
      "INFO:root:City 'miami' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Toronto, Canada ===\n",
      "INFO:root:City 'toronto' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Madrid, Spain ===\n",
      "INFO:root:City 'madrid' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Santiago, Chile ===\n",
      "INFO:root:City 'santiago' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Houston, United States ===\n",
      "INFO:root:City 'houston' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Riyadh Governorate, Saudi Arabia ===\n",
      "INFO:root:City 'riyadh_governorate' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Baghdad, Iraq ===\n",
      "INFO:root:City 'baghdad' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Dar es Salaam, Tanzania ===\n",
      "INFO:root:City 'dar_es_salaam' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Kuala Lumpur, Malaysia ===\n",
      "INFO:root:City 'kuala_lumpur' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Chicago, United States ===\n",
      "INFO:root:City 'chicago' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Tehran, Iran ===\n",
      "INFO:root:City 'tehran' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Bangkok, Thailand ===\n",
      "INFO:root:City 'bangkok' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Provincia de Lima, Peru ===\n",
      "INFO:root:City 'provincia_de_lima' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Paris, France ===\n",
      "INFO:root:City 'paris' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Bogotá, Colombia ===\n",
      "INFO:root:City 'bogota' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Chennai, India ===\n",
      "INFO:root:City 'chennai' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Ho Chi Minh City, Vietnam ===\n",
      "INFO:root:City 'ho_chi_minh_city' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Manila, Philippines ===\n",
      "INFO:root:City 'manila' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: London, United Kingdom ===\n",
      "INFO:root:City 'london' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Buenos Aires, Argentina ===\n",
      "INFO:root:City 'buenos_aires' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Istanbul, Turkey ===\n",
      "INFO:root:City 'istanbul' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Lagos, Nigeria ===\n",
      "INFO:root:City 'lagos' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Karachi, Pakistan ===\n",
      "INFO:root:City 'karachi' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Moscow, Russia ===\n",
      "INFO:root:City 'moscow' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Los Angeles, United States ===\n",
      "INFO:root:City 'los_angeles' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: New York, United States ===\n",
      "INFO:root:City 'new_york' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Mumbai, India ===\n",
      "INFO:root:City 'mumbai' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Beijing, China ===\n",
      "INFO:root:City 'beijing' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Mexico City, Mexico ===\n",
      "INFO:root:City 'mexico_city' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Cairo, Egypt ===\n",
      "INFO:root:City 'cairo' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Município de São Paulo, Brazil ===\n",
      "INFO:root:City 'municipio_de_sao_paulo' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Dhaka, Bangladesh ===\n",
      "INFO:root:City 'dhaka' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Shanghai, China ===\n",
      "INFO:root:City 'shanghai' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Delhi, India ===\n",
      "INFO:root:City 'delhi' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Jakarta, Indonesia ===\n",
      "INFO:root:City 'jakarta' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Tokyo, Japan ===\n",
      "INFO:root:City 'tokyo' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Coimbra, Portugal ===\n",
      "INFO:root:City 'coimbra' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Fortaleza, Brazil ===\n",
      "INFO:root:City 'fortaleza' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Processing City: Hamburg, Germany ===\n",
      "INFO:root:City 'hamburg' skipped (already processed according to log).\n",
      "INFO:root:\n",
      "=== Total Script Execution Time: 0.05 seconds ===\n",
      "INFO:root:Final city processing times saved to /Users/Matheus/Library/CloudStorage/OneDrive-Pessoal/Estudos/2 - Pesquisas/3 - CITTA/1 - RAMCCAV/3 - Assessment of Physical Road Infrastructure/Codigos/results/city_processing_times.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_inputs = [\n",
    "    \"Nuku'alofa, Tonga\",\n",
    "    \"Apia, Samoa\",\n",
    "    \"Honiara, Solomon Islands\",\n",
    "    \"Reykjavik, Iceland\",\n",
    "    \"Suva, Fiji\",\n",
    "    \"National Capital District, Papua New Guinea\",\n",
    "    \"Wellington City, New Zealand\",\n",
    "    \"Kigali, Rwanda\",\n",
    "    \"Oslo, Norway\",\n",
    "    \"Zurich, Switzerland\",\n",
    "    \"San José, Costa Rica\",\n",
    "    \"Auckland, New Zealand\",\n",
    "    \"Sofia, Bulgaria\",\n",
    "    \"Belgrade, Serbia\",\n",
    "    \"Montevideo, Uruguay\",\n",
    "    \"Calgary, Canada\",\n",
    "    \"Distrito de Panamá, Panama\",\n",
    "    \"Vienna, Austria\",\n",
    "    \"Dublin, Ireland\",\n",
    "    \"Perth, Australia\",\n",
    "    \"Prague, Czech Republic\",\n",
    "    \"Bucharest, Romania\",\n",
    "    \"Brussels, Belgium\",\n",
    "    \"La Paz, Bolivia\",\n",
    "    \"Baku, Azerbaijan\",\n",
    "    \"Doha, Qatar\",\n",
    "    \"Beirut, Lebanon\",\n",
    "    \"Amsterdam, Netherlands\",\n",
    "    \"Brisbane, Australia\",\n",
    "    \"Vancouver, Canada\",\n",
    "    \"Ciudad de Túnez, Tunisia\",\n",
    "    \"Caracas, Venezuela\",\n",
    "    \"Budapest, Hungary\",\n",
    "    \"Tashkent, Uzbekistan\",\n",
    "    \"Kyiv, Ukraine\",\n",
    "    \"Warsaw, Poland\",\n",
    "    \"Quito, Ecuador\",\n",
    "    \"Dakar, Senegal\",\n",
    "    \"Lusaka, Zambia\",\n",
    "    \"Dubai, United Arab Emirates\",\n",
    "    \"Algiers, Algeria\",\n",
    "    \"Casablanca, Morocco\",\n",
    "    \"Kampala, Uganda\",\n",
    "    \"Medellín, Colombia\",\n",
    "    \"Berlin, Germany\",\n",
    "    \"Rome, Italy\",\n",
    "    \"Montreal, Canada\",\n",
    "    \"Amman, Jordan\",\n",
    "    \"San Francisco, United States\",\n",
    "    \"City of Cape Town, South Africa\",\n",
    "    \"Boston, United States\",\n",
    "    \"Accra, Ghana\",\n",
    "    \"Monterrey, Mexico\",\n",
    "    \"Nairobi, Kenya\",\n",
    "    \"Melbourne, Australia\",\n",
    "    \"Guadalajara, Mexico\",\n",
    "    \"Sydney, Australia\",\n",
    "    \"Alexandria, Egypt\",\n",
    "    \"Colombo, Sri Lanka\",\n",
    "    \"Yangon, Myanmar\",\n",
    "    \"Singapore, Singapore\",\n",
    "    \"Miami, United States\",\n",
    "    \"Toronto, Canada\",\n",
    "    \"Madrid, Spain\",\n",
    "    \"Santiago, Chile\",\n",
    "    \"Houston, United States\",\n",
    "    \"Riyadh Governorate, Saudi Arabia\",\n",
    "    \"Baghdad, Iraq\",\n",
    "    \"Dar es Salaam, Tanzania\",\n",
    "    \"Kuala Lumpur, Malaysia\",\n",
    "    \"Chicago, United States\",\n",
    "    \"Tehran, Iran\",\n",
    "    \"Bangkok, Thailand\",\n",
    "    \"Provincia de Lima, Peru\",\n",
    "    \"Paris, France\",\n",
    "    \"Bogotá, Colombia\",\n",
    "    \"Chennai, India\",\n",
    "    \"Ho Chi Minh City, Vietnam\",\n",
    "    \"Manila, Philippines\",\n",
    "    \"London, United Kingdom\",\n",
    "    \"Buenos Aires, Argentina\",\n",
    "    \"Istanbul, Turkey\",\n",
    "    \"Lagos, Nigeria\",\n",
    "    \"Karachi, Pakistan\",\n",
    "    \"Moscow, Russia\",\n",
    "    \"Los Angeles, United States\",\n",
    "    \"New York, United States\",\n",
    "    \"Mumbai, India\",\n",
    "    \"Beijing, China\",\n",
    "    \"Mexico City, Mexico\",\n",
    "    \"Cairo, Egypt\",\n",
    "    \"Município de São Paulo, Brazil\",\n",
    "    \"Dhaka, Bangladesh\",\n",
    "    \"Shanghai, China\",\n",
    "    \"Delhi, India\",\n",
    "    \"Jakarta, Indonesia\",\n",
    "    \"Tokyo, Japan\",\n",
    "    \"Coimbra, Portugal\",\n",
    "    \"Fortaleza, Brazil\",\n",
    "    \"Hamburg, Germany\",\n",
    "]\n",
    "    main(user_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7ef8fb1-c26d-4ae7-a722-f31c5cd1db50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Final Aggregation of All Processed Networks ===\n",
      "Found 100 network files to merge from '/Users/Matheus/Library/CloudStorage/OneDrive-Pessoal/Estudos/2 - Pesquisas/3 - CITTA/1 - RAMCCAV/3 - Assessment of Physical Road Infrastructure/Codigos/data'.\n",
      "Reading: zurich (zurich_processed_network.gpkg)\n",
      "Reading: calgary (calgary_processed_network.gpkg)\n",
      "Reading: new_york (new_york_processed_network.gpkg)\n",
      "Reading: paris (paris_processed_network.gpkg)\n",
      "Reading: dakar (dakar_processed_network.gpkg)\n",
      "Reading: mumbai (mumbai_processed_network.gpkg)\n",
      "Reading: riyadh_governorate (riyadh_governorate_processed_network.gpkg)\n",
      "Reading: karachi (karachi_processed_network.gpkg)\n",
      "Reading: santiago (santiago_processed_network.gpkg)\n",
      "Reading: ciudad_de_tunez (ciudad_de_tunez_processed_network.gpkg)\n",
      "Reading: melbourne (melbourne_processed_network.gpkg)\n",
      "Reading: beijing (beijing_processed_network.gpkg)\n",
      "Reading: moscow (moscow_processed_network.gpkg)\n",
      "Reading: municipio_de_sao_paulo (municipio_de_sao_paulo_processed_network.gpkg)\n",
      "Reading: casablanca (casablanca_processed_network.gpkg)\n",
      "Reading: medellin (medellin_processed_network.gpkg)\n",
      "Reading: algiers (algiers_processed_network.gpkg)\n",
      "Reading: amman (amman_processed_network.gpkg)\n",
      "Reading: dublin (dublin_processed_network.gpkg)\n",
      "Reading: apia (apia_processed_network.gpkg)\n",
      "Reading: lagos (lagos_processed_network.gpkg)\n",
      "Reading: manila (manila_processed_network.gpkg)\n",
      "Reading: chicago (chicago_processed_network.gpkg)\n",
      "Reading: belgrade (belgrade_processed_network.gpkg)\n",
      "Reading: tashkent (tashkent_processed_network.gpkg)\n",
      "Reading: bogota (bogota_processed_network.gpkg)\n",
      "Reading: chennai (chennai_processed_network.gpkg)\n",
      "Reading: guadalajara (guadalajara_processed_network.gpkg)\n",
      "Reading: san_jose (san_jose_processed_network.gpkg)\n",
      "Reading: delhi (delhi_processed_network.gpkg)\n",
      "Reading: tokyo (tokyo_processed_network.gpkg)\n",
      "Reading: montreal (montreal_processed_network.gpkg)\n",
      "Reading: baku (baku_processed_network.gpkg)\n",
      "Reading: provincia_de_lima (provincia_de_lima_processed_network.gpkg)\n",
      "Reading: istanbul (istanbul_processed_network.gpkg)\n",
      "Reading: sydney (sydney_processed_network.gpkg)\n",
      "Reading: quito (quito_processed_network.gpkg)\n",
      "Reading: ho_chi_minh_city (ho_chi_minh_city_processed_network.gpkg)\n",
      "Reading: coimbra (coimbra_processed_network.gpkg)\n",
      "Reading: yangon (yangon_processed_network.gpkg)\n",
      "Reading: houston (houston_processed_network.gpkg)\n",
      "Reading: la_paz (la_paz_processed_network.gpkg)\n",
      "Reading: nairobi (nairobi_processed_network.gpkg)\n",
      "Reading: cairo (cairo_processed_network.gpkg)\n",
      "Reading: monterrey (monterrey_processed_network.gpkg)\n",
      "Reading: madrid (madrid_processed_network.gpkg)\n",
      "Reading: national_capital_district (national_capital_district_processed_network.gpkg)\n",
      "Reading: colombo (colombo_processed_network.gpkg)\n",
      "Reading: kampala (kampala_processed_network.gpkg)\n",
      "Reading: doha (doha_processed_network.gpkg)\n",
      "Reading: jakarta (jakarta_processed_network.gpkg)\n",
      "Reading: hamburg (hamburg_processed_network.gpkg)\n",
      "Reading: brisbane (brisbane_processed_network.gpkg)\n",
      "Reading: san_francisco (san_francisco_processed_network.gpkg)\n",
      "Reading: boston (boston_processed_network.gpkg)\n",
      "Reading: city_of_cape_town (city_of_cape_town_processed_network.gpkg)\n",
      "Reading: honiara (honiara_processed_network.gpkg)\n",
      "Reading: vienna (vienna_processed_network.gpkg)\n",
      "Reading: budapest (budapest_processed_network.gpkg)\n",
      "Reading: vancouver (vancouver_processed_network.gpkg)\n",
      "Reading: singapore (singapore_processed_network.gpkg)\n",
      "Reading: alexandria (alexandria_processed_network.gpkg)\n",
      "Reading: fortaleza (fortaleza_processed_network.gpkg)\n",
      "Reading: buenos_aires (buenos_aires_processed_network.gpkg)\n",
      "Reading: lusaka (lusaka_processed_network.gpkg)\n",
      "Reading: los_angeles (los_angeles_processed_network.gpkg)\n",
      "Reading: perth (perth_processed_network.gpkg)\n",
      "Reading: auckland (auckland_processed_network.gpkg)\n",
      "Reading: dhaka (dhaka_processed_network.gpkg)\n",
      "Reading: oslo (oslo_processed_network.gpkg)\n",
      "Reading: kyiv (kyiv_processed_network.gpkg)\n",
      "Reading: kuala_lumpur (kuala_lumpur_processed_network.gpkg)\n",
      "Reading: toronto (toronto_processed_network.gpkg)\n",
      "Reading: accra (accra_processed_network.gpkg)\n",
      "Reading: tehran (tehran_processed_network.gpkg)\n",
      "Reading: beirut (beirut_processed_network.gpkg)\n",
      "Reading: reykjavik (reykjavik_processed_network.gpkg)\n",
      "Reading: london (london_processed_network.gpkg)\n",
      "Reading: miami (miami_processed_network.gpkg)\n",
      "Reading: dubai (dubai_processed_network.gpkg)\n",
      "Reading: caracas (caracas_processed_network.gpkg)\n",
      "Reading: prague (prague_processed_network.gpkg)\n",
      "Reading: berlin (berlin_processed_network.gpkg)\n",
      "Reading: rome (rome_processed_network.gpkg)\n",
      "Reading: nukualofa (nukualofa_processed_network.gpkg)\n",
      "Reading: shanghai_backup (shanghai_processed_network.gpkg)\n",
      "Reading: distrito_de_panama (distrito_de_panama_processed_network.gpkg)\n",
      "Reading: amsterdam (amsterdam_processed_network.gpkg)\n",
      "Reading: montevideo (montevideo_processed_network.gpkg)\n",
      "Reading: dar_es_salaam (dar_es_salaam_processed_network.gpkg)\n",
      "Reading: baghdad (baghdad_processed_network.gpkg)\n",
      "Reading: kigali (kigali_processed_network.gpkg)\n",
      "Reading: bucharest (bucharest_processed_network.gpkg)\n",
      "Reading: suva (suva_processed_network.gpkg)\n",
      "Reading: mexico_city (mexico_city_processed_network.gpkg)\n",
      "Reading: bangkok (bangkok_processed_network.gpkg)\n",
      "Reading: sofia (sofia_processed_network.gpkg)\n",
      "Reading: wellington_city (wellington_city_processed_network.gpkg)\n",
      "Reading: brussels (brussels_processed_network.gpkg)\n",
      "Reading: warsaw (warsaw_processed_network.gpkg)\n",
      "\n",
      "Successfully saved the final merged network to: /Users/Matheus/Library/CloudStorage/OneDrive-Pessoal/Estudos/2 - Pesquisas/3 - CITTA/1 - RAMCCAV/3 - Assessment of Physical Road Infrastructure/Codigos/results/full_network.gpkg\n",
      "Total Edges in Merged Network: 7002507\n",
      "Now saving as CSV to: /Users/Matheus/Library/CloudStorage/OneDrive-Pessoal/Estudos/2 - Pesquisas/3 - CITTA/1 - RAMCCAV/3 - Assessment of Physical Road Infrastructure/Codigos/results/full_network.csv\n",
      "Successfully saved CSV version of the network.\n"
     ]
    }
   ],
   "source": [
    "# After the main function has successfully created all individual city networks,\n",
    "# this code will merge them into a single file.\n",
    "\n",
    "print(\"\\n=== Starting Final Aggregation of All Processed Networks ===\")\n",
    "\n",
    "# Set this to True to also save a CSV file of the final network\n",
    "SAVE_AS_CSV = True\n",
    "\n",
    "# Find all the processed network files using the correct DATA_DIR path\n",
    "processed_network_files = glob.glob(os.path.join(DATA_DIR, \"*\", \"*_processed_network.gpkg\"))\n",
    "\n",
    "if not processed_network_files:\n",
    "    print(f\"No processed network files found in '{DATA_DIR}'. Please check the path and ensure the main script ran successfully.\")\n",
    "else:\n",
    "    print(f\"Found {len(processed_network_files)} network files to merge from '{DATA_DIR}'.\")\n",
    "    \n",
    "    # List to hold the individual GeoDataFrames\n",
    "    all_networks_list = []\n",
    "    \n",
    "    for f in processed_network_files:\n",
    "        try:\n",
    "            # Extract city_id from the file path to add as a column\n",
    "            city_id = os.path.basename(os.path.dirname(f))\n",
    "            print(f\"Reading: {city_id} ({os.path.basename(f)})\")\n",
    "            gdf = gpd.read_file(f)\n",
    "            gdf['city_id'] = city_id # Add a column to identify the city\n",
    "            all_networks_list.append(gdf)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read or process file {f}. Error: {e}\")\n",
    "\n",
    "    if all_networks_list:\n",
    "        # Concatenate all GeoDataFrames in the list\n",
    "        final_merged_network = pd.concat(all_networks_list, ignore_index=True)\n",
    "        \n",
    "        output_path_gpkg = os.path.join(RESULTS_DIR, \"full_network.gpkg\")\n",
    "        output_path_csv = os.path.join(RESULTS_DIR, \"full_network.csv\")\n",
    "        \n",
    "        # --- Save the primary GeoPackage (.gpkg) file ---\n",
    "        try:\n",
    "            final_merged_network.to_file(output_path_gpkg, driver=\"GPKG\")\n",
    "            print(f\"\\nSuccessfully saved the final merged network to: {output_path_gpkg}\")\n",
    "            print(f\"Total Edges in Merged Network: {len(final_merged_network)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save the final merged GPKG network. Error: {e}\")\n",
    "\n",
    "        # --- Optionally, save the network as a CSV file ---\n",
    "        if SAVE_AS_CSV:\n",
    "            print(f\"Now saving as CSV to: {output_path_csv}\")\n",
    "            try:\n",
    "                final_merged_network.to_csv(output_path_csv, index=False, sep=';')\n",
    "                print(f\"Successfully saved CSV version of the network.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save the CSV network. Error: {e}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No networks were successfully read, so no final file was created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affbc35-0177-4914-9f68-3a2b640dbd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
